{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "from typing import List, Dict, Tuple, Union\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('nerc_datasets/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features: List[Dict[str,str]] = []\n",
    "training_gold_labels: List[str] = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict: Dict[str, str] = {\n",
    "       'words': token, \n",
    "       'pos': pos\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.COLUMN_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader('nerc_datasets/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features: List[Dict[str,str]] = []\n",
    "test_gold_labels: List[str] = []\n",
    "\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict: Dict[str, str] = {\n",
    "       'words': token, \n",
    "       'pos': pos\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances TRAINING SET: 203621, Total instances TEST SET: 46435.\n",
      "\n",
      "Label: O, Absolute frequency TRAINING SET: 169578, Relative frequency TRAINING SET: 83.28%.\n",
      "Label: O, Absolute frequency TEST SET: 38323, Relative frequency TEST SET: 82.53%.\n",
      "\n",
      "Label: B-LOC, Absolute frequency TRAINING SET: 7140, Relative frequency TRAINING SET: 3.51%.\n",
      "Label: B-LOC, Absolute frequency TEST SET: 1668, Relative frequency TEST SET: 3.59%.\n",
      "\n",
      "Label: B-PER, Absolute frequency TRAINING SET: 6600, Relative frequency TRAINING SET: 3.24%.\n",
      "Label: B-ORG, Absolute frequency TEST SET: 1661, Relative frequency TEST SET: 3.58%.\n",
      "\n",
      "Label: B-ORG, Absolute frequency TRAINING SET: 6321, Relative frequency TRAINING SET: 3.10%.\n",
      "Label: B-PER, Absolute frequency TEST SET: 1617, Relative frequency TEST SET: 3.48%.\n",
      "\n",
      "Label: I-PER, Absolute frequency TRAINING SET: 4528, Relative frequency TRAINING SET: 2.22%.\n",
      "Label: I-PER, Absolute frequency TEST SET: 1156, Relative frequency TEST SET: 2.49%.\n",
      "\n",
      "Label: I-ORG, Absolute frequency TRAINING SET: 3704, Relative frequency TRAINING SET: 1.82%.\n",
      "Label: I-ORG, Absolute frequency TEST SET: 835, Relative frequency TEST SET: 1.80%.\n",
      "\n",
      "Label: B-MISC, Absolute frequency TRAINING SET: 3438, Relative frequency TRAINING SET: 1.69%.\n",
      "Label: B-MISC, Absolute frequency TEST SET: 702, Relative frequency TEST SET: 1.51%.\n",
      "\n",
      "Label: I-LOC, Absolute frequency TRAINING SET: 1157, Relative frequency TRAINING SET: 0.57%.\n",
      "Label: I-LOC, Absolute frequency TEST SET: 257, Relative frequency TEST SET: 0.55%.\n",
      "\n",
      "Label: I-MISC, Absolute frequency TRAINING SET: 1155, Relative frequency TRAINING SET: 0.57%.\n",
      "Label: I-MISC, Absolute frequency TEST SET: 216, Relative frequency TEST SET: 0.47%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "\n",
    "num_train:int = len(training_features) # number of instances in the training set\n",
    "num_test:int = len(test_features) # number of instances in the test set\n",
    "\n",
    "freq_dist_train = Counter(training_gold_labels)\n",
    "freq_dist_test = Counter(test_gold_labels)\n",
    "\n",
    "freq_dist_train: Dict[str, int] = dict(sorted(freq_dist_train.items(), key=lambda item: item[1], reverse=True)) # The sorted frequency distribution dictionary of the training set (high to low)\n",
    "freq_dist_test: Dict[str, int] = dict(sorted(freq_dist_test.items(), key=lambda item: item[1], reverse=True)) # The sorted frequency distribution dictionary of the test set (high to low)\n",
    "\n",
    "print(f'Total instances TRAINING SET: {num_train}, Total instances TEST SET: {num_test}.')\n",
    "print()\n",
    "\n",
    "for i,j in zip(freq_dist_train, freq_dist_test):\n",
    "    print(f'Label: {i}, Absolute frequency TRAINING SET: {freq_dist_train[i]}, Relative frequency TRAINING SET: {(freq_dist_train[i]/num_train)*100:.2f}%.')\n",
    "    print(f'Label: {j}, Absolute frequency TEST SET: {freq_dist_test[j]}, Relative frequency TEST SET: {(freq_dist_test[j]/num_test)*100:.2f}%.')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2b\n",
    "While the datasets exhibit some imbalance with certain labels having significantly fewer instances compared to others, the distribution of NERC labels remains relatively consistent across both datasets. In both the training and test data, a majority of instances are labeled as 'O' (Other) with a relative frequency of 83.28% in the training set and 82.53% in the test set.\n",
    "\n",
    "Following this, the next three labels ('B-LOC', 'B-PER', 'B-ORG', and 'I-PER') demonstrate similar relative frequencies ranging between 3.51% and 2.22% in both datasets. Conversely, the least frequent labels ('I-ORG', 'B-MISC', 'I-LOC', and 'I-MISC') are observed with lower relative frequencies, occurring between 1.82% and 0.47% of the cases in both datasets.\n",
    "\n",
    "Despite slight disparities in the absolute frequencies of labels, the overall distribution pattern remains quite consistent between the training and test datasets. This consistency is crucial for ensuring the robustness and generalizability of the Named Entity Recognition (NER) model across various datasets and scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer(sparse=True)\n",
    "train_test = training_features + test_features\n",
    "the_array = vec.fit_transform(train_test)\n",
    "\n",
    "train_array = the_array[:num_train]\n",
    "test_array = the_array[num_train:]\n",
    "# print(train_array.shape)\n",
    "# print(test_array.shape)\n",
    "# print(the_array.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.812     0.775     0.793      1668\n",
      "      B-MISC      0.782     0.664     0.718       702\n",
      "       B-ORG      0.792     0.519     0.627      1661\n",
      "       B-PER      0.860     0.437     0.579      1617\n",
      "       I-LOC      0.618     0.529     0.570       257\n",
      "      I-MISC      0.570     0.588     0.579       216\n",
      "       I-ORG      0.703     0.467     0.561       835\n",
      "       I-PER      0.333     0.871     0.481      1156\n",
      "           O      0.985     0.984     0.985     38323\n",
      "\n",
      "    accuracy                          0.920     46435\n",
      "   macro avg      0.717     0.648     0.655     46435\n",
      "weighted avg      0.939     0.920     0.922     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lin_clf.fit(train_array, training_gold_labels) # training\n",
    "pred = lin_clf.predict(test_array) # testing\n",
    "\n",
    "report = classification_report(test_gold_labels, pred ,digits = 3) # evaluation\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4d:\n",
    "**O**: The high precision and recall values for the non-entity class suggest that the classifier performs well in identifying tokens that do not belong to any named entity category, indicating robust learning of non-entity patterns. This is likely due to their over-represention in the training set.<br>\n",
    "**B-LOC**: The relatively high precision and recall suggest that location entities often have distinct patterns or keywords, making them easier for the classifier to recognize accurately. <br>\n",
    "**B-MISC**:  Miscellaneous entities cover a wide range of categories and can be ambiguous depending on the context. The relatively lower recall may indicate the presence of diverse and less recognizable patterns for miscellaneous entities.<br>\n",
    "**B-ORG**: Organizations often have unique naming conventions or structures, contributing to the relatively high precision. However, the lower recall suggests that some organization entities may have less distinct patterns or may be less prevalent in the data. <br>\n",
    "**B-PER**: Person names may follow certain recognizable patterns, contributing to the high precision. However, the lower recall suggests that some person entities may be less predictable or less prevalent in the data.<br>\n",
    "**I-LOC**:  Inside location entities often occur within the context of longer phrases or sentences, where the presence of multiple words and syntactic structures can introduce ambiguity and variability. Therefore, inside location entities may have less distinct patterns compared to beginning of location entities, leading to slightly lower precision(0.618) and recall (0.529) values. <br>\n",
    "**I-ORG**:  Inside organization entities may exhibit less distinct patterns compared to beginning of organization entities, leading to lower recall values.<br>\n",
    "**I-PER**: The low precision (0.333) suggests that the classifier struggles to accurately predict inside person entities, possibly due to less recognizable patterns. However, the high recall (0.871) indicates that it effectively captures most actual inside person entities. This discrepancy suggests that although the classifier is effective at capturing most instances of inside person entities, it also tends to generate a significant number of false positive predictions, likely because inside person entities exhibit less recognizable patterns and may share similarities with other entity types or non-entity tokens.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=500000) \n",
    "\n",
    "training_input= []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    word=token #the next word from the tokenized text\n",
    "    # we check if our word \n",
    "    # is inside the model vocabulary (loaded with the Google word2vec embeddings)\n",
    "    if word in word_embedding_model:\n",
    "        # in this case the word was found and vector is assigned with its embedding vector as the value\n",
    "        vector=word_embedding_model[word]\n",
    "    else: \n",
    "        # if the word does not exist in the embeddings vocabulary, \n",
    "        # we create a vector with all zeros.\n",
    "        # The Google word2vec model has 300 dimensions so we creat a vector with 300 zeros\n",
    "        vector=[0]*300\n",
    "        # print('This word is not in the word2vec vocabulary:', word)\n",
    "    training_input.append(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    word=token #the next word from the tokenized text\n",
    "    # we check if our word \n",
    "    # is inside the model vocabulary (loaded with the Google word2vec embeddings)\n",
    "    if word in word_embedding_model:\n",
    "        # in this case the word was found and vector is assigned with its embedding vector as the value\n",
    "        vector=word_embedding_model[word]\n",
    "    else: \n",
    "        # if the word does not exist in the embeddings vocabulary, \n",
    "        # we create a vector with all zeros.\n",
    "        # The Google word2vec model has 300 dimensions so we creat a vector with 300 zeros\n",
    "        vector=[0]*300\n",
    "        # print('This word is not in the word2vec vocabulary:', word)\n",
    "    test_input.append(vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf_emb = svm.LinearSVC()\n",
    "lin_clf_emb.fit(training_input, training_gold_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.759     0.796     0.777      1668 (embedded)\n",
      "       B-LOC      0.812     0.775     0.793      1668\n",
      "\n",
      "      B-MISC      0.724     0.688     0.706       702 (embedded)\n",
      "      B-MISC      0.782     0.664     0.718       702\n",
      "\n",
      "       B-ORG      0.700     0.635     0.666      1661 (embedded)\n",
      "       B-ORG      0.792     0.519     0.627      1661\n",
      "\n",
      "       B-PER      0.760     0.635     0.692      1617 (embedded)\n",
      "       B-PER      0.860     0.437     0.579      1617\n",
      "\n",
      "       I-LOC      0.522     0.412     0.461       257 (embedded)\n",
      "       I-LOC      0.618     0.529     0.570       257\n",
      "\n",
      "      I-MISC      0.595     0.523     0.557       216 (embedded)\n",
      "      I-MISC      0.570     0.588     0.579       216\n",
      "\n",
      "       I-ORG      0.498     0.344     0.407       835 (embedded)\n",
      "       I-ORG      0.703     0.467     0.561       835\n",
      "\n",
      "       I-PER      0.591     0.454     0.513      1156 (embedded)\n",
      "       I-PER      0.333     0.871     0.481      1156\n",
      "\n",
      "           O      0.967     0.991     0.979     38323 (embedded)\n",
      "           O      0.985     0.984     0.985     38323\n",
      "\n",
      "\n",
      "    accuracy                          0.924     46435 (embedded)\n",
      "    accuracy                          0.920     46435\n",
      "\n",
      "   macro avg      0.679     0.609     0.640     46435 (embedded)\n",
      "   macro avg      0.717     0.648     0.655     46435\n",
      "\n",
      "weighted avg      0.917     0.924     0.920     46435 (embedded)\n",
      "weighted avg      0.939     0.920     0.922     46435\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_emb = lin_clf_emb.predict(test_input)\n",
    "report_emb = classification_report(test_gold_labels, pred_emb ,digits = 3) # evaluation\n",
    "\n",
    "# print(report_emb)\n",
    "report_cols = report_emb.split('\\n')[0]\n",
    "report_emb_lines = report_emb.split('\\n')[1:]\n",
    "report_lines = report.split('\\n')[1:]\n",
    "\n",
    "print(report_cols)\n",
    "for line_emb, line in zip(report_emb_lines, report_lines):\n",
    "    if line_emb != '':\n",
    "        print(line_emb, '(embedded)')\n",
    "        print(line)\n",
    "        print()\n",
    "    else:\n",
    "        print(line)\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mling\\AppData\\Local\\Temp\\ipykernel_26184\\1318043532.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)\n",
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = 'nerc_datasets/kaggle/ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050795"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>...</th>\n",
       "      <th>prev-prev-lemma</th>\n",
       "      <th>prev-prev-pos</th>\n",
       "      <th>prev-prev-shape</th>\n",
       "      <th>prev-prev-word</th>\n",
       "      <th>prev-shape</th>\n",
       "      <th>prev-word</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>shape</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thousand</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>...</td>\n",
       "      <td>__start2__</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>...</td>\n",
       "      <td>__start1__</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>...</td>\n",
       "      <td>thousand</td>\n",
       "      <td>NNS</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>...</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>...</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>london</td>\n",
       "      <td>to</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>protest</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>...</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>to</td>\n",
       "      <td>protest</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>VB</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>protest</td>\n",
       "      <td>...</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>protest</td>\n",
       "      <td>the</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>war</td>\n",
       "      <td>DT</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>...</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>protest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the</td>\n",
       "      <td>war</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>in</td>\n",
       "      <td>NN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>war</td>\n",
       "      <td>...</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>protest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     lemma next-lemma next-next-lemma next-next-pos next-next-shape  \\\n",
       "0   0  thousand         of        demonstr           NNS       lowercase   \n",
       "1   1        of   demonstr            have           VBP       lowercase   \n",
       "2   2  demonstr       have           march           VBN       lowercase   \n",
       "3   3      have      march         through            IN       lowercase   \n",
       "4   4     march    through          london           NNP     capitalized   \n",
       "5   5   through     london              to            TO       lowercase   \n",
       "6   6    london         to         protest            VB       lowercase   \n",
       "7   7        to    protest             the            DT       lowercase   \n",
       "8   8   protest        the             war            NN       lowercase   \n",
       "9   9       the        war              in            IN       lowercase   \n",
       "\n",
       "  next-next-word next-pos   next-shape      next-word  ... prev-prev-lemma  \\\n",
       "0  demonstrators       IN    lowercase             of  ...      __start2__   \n",
       "1           have      NNS    lowercase  demonstrators  ...      __start1__   \n",
       "2        marched      VBP    lowercase           have  ...        thousand   \n",
       "3        through      VBN    lowercase        marched  ...              of   \n",
       "4         London       IN    lowercase        through  ...        demonstr   \n",
       "5             to      NNP  capitalized         London  ...            have   \n",
       "6        protest       TO    lowercase             to  ...           march   \n",
       "7            the       VB    lowercase        protest  ...         through   \n",
       "8            war       DT    lowercase            the  ...          london   \n",
       "9             in       NN    lowercase            war  ...              to   \n",
       "\n",
       "  prev-prev-pos prev-prev-shape prev-prev-word   prev-shape      prev-word  \\\n",
       "0    __START2__        wildcard     __START2__     wildcard     __START1__   \n",
       "1    __START1__        wildcard     __START1__  capitalized      Thousands   \n",
       "2           NNS     capitalized      Thousands    lowercase             of   \n",
       "3            IN       lowercase             of    lowercase  demonstrators   \n",
       "4           NNS       lowercase  demonstrators    lowercase           have   \n",
       "5           VBP       lowercase           have    lowercase        marched   \n",
       "6           VBN       lowercase        marched    lowercase        through   \n",
       "7            IN       lowercase        through  capitalized         London   \n",
       "8           NNP     capitalized         London    lowercase             to   \n",
       "9            TO       lowercase             to    lowercase        protest   \n",
       "\n",
       "  sentence_idx        shape           word    tag  \n",
       "0          1.0  capitalized      Thousands      O  \n",
       "1          1.0    lowercase             of      O  \n",
       "2          1.0    lowercase  demonstrators      O  \n",
       "3          1.0    lowercase           have      O  \n",
       "4          1.0    lowercase        marched      O  \n",
       "5          1.0    lowercase        through      O  \n",
       "6          1.0  capitalized         London  B-geo  \n",
       "7          1.0    lowercase             to      O  \n",
       "8          1.0    lowercase        protest      O  \n",
       "9          1.0    lowercase            the      O  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = []\n",
    "training_labels = []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    features = {}  \n",
    "\n",
    "    for column in df_train.columns:\n",
    "        \n",
    "        features[column] = row[column]  # Adds each feature to the dict\n",
    "        \n",
    "    training_features.append(features)\n",
    "    training_labels.append(row['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    features = {}  \n",
    "\n",
    "    for column in df_test.columns:\n",
    "        \n",
    "        features[column] = row[column]  # Adds each feature to the dict\n",
    "        \n",
    "    test_features.append(features)\n",
    "    test_labels.append(row['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<203621x27361 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 407242 stored elements in Compressed Sparse Row format>]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "all_features = training_features + test_features\n",
    "\n",
    "vec = DictVectorizer(sparse=True)\n",
    "array = vec.fit_transform(all_features)\n",
    "\n",
    "train_arr = array[:100000]\n",
    "test_arr = array[100000:]\n",
    "\n",
    "print(numpy.unique(train_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mling\\anaconda3\\envs\\envpython3.8\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf_kaggle = svm.LinearSVC()\n",
    "lin_clf_kaggle.fit(train_arr, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (envpython3.8)",
   "language": "python",
   "name": "python38_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
