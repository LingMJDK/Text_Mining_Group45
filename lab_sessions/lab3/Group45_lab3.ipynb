{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Sentence 1: The presence of the word \"love\" correctly influences a positive sententiment ('pos: 0.808', compound: '0.6369'). The word love has a high positive valence in the lexicon.\n",
    "\n",
    "Sentence 2: VADER implements the negatiion rule in this sentence, flipping the sentiment valence of 'love' to negative  (neg: 0.627, compound: -0.5216) . \n",
    "\n",
    "Sentence 3: The positive sentiment is even stronger (pos: 0.867, compound: 0.7579) than in sentence 1, correctly influenced by the emoticon \":-)\" which adds to the positive sentiment. Emoticons are included in VADER's lexicon. \n",
    "\n",
    "Sentence 4: The negative sentiment (neg: 0.492, compound: -0.4404) comes from the word 'ruins' which can have negative connotations. This assignment might not always fit the context for example historical ruins.\n",
    "\n",
    "Sentence 5: The negation \"not\" correctly influences the sentiment to be more positive (pos: 0.49, compound: 0.5867), suggesting VADER's effective handling of negation in conjunction with an intensifier \"certainly\" leading to a high positive score. However with different context it could be the negative sentiment influence of 'ruins' could again make this sentence sentiment be inaccurate.\n",
    "\n",
    "Sentence 6: The negative sentiment (neg: 0.286, compound: -0.4215) is incorrectly applied. This is likely because \"lies\" is interpreted as dishonesty, whereas in this context, it simply means reclining.\n",
    "\n",
    "Sentence 7: The slightly positive sentiment (pos: 0.333, compound: 0.3612) might be unexpected as the sentence appears neutral. This could be due to the lack of contextually negative or positive cues, leading to a default slight positive interpretation. Without clear sentiment-laden words, the analysis leans slightly positive, which might reflect an inherent bias or the treatment of neutral statements in VADER's algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'neutral', 'text_of_tweet': '@BritIndianVoice @RishiSunak #RishiSunak is #BritishHindu of paternal #Pakistani Punjabi origin. He‚Äôs legally POC (#Pakistan\\xa0Origin Card) holder issued by @NadraPak (probably also concurrently holding India‚Äôs OCI through his Indian NRI wife/mother). Also there may be issue with his wife‚Äôs Indian citizenship.', 'tweet_url': 'https://twitter.com/IsmailYSyed/status/1584696134693711873'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "\n",
    "#copied from lab3.2\n",
    "\n",
    "def run_vader(textual_unit, \n",
    "              lemmatize=True, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.500     0.350     0.412        20\n",
      "     neutral      0.700     0.389     0.500        18\n",
      "    positive      0.308     0.667     0.421        12\n",
      "\n",
      "    accuracy                          0.440        50\n",
      "   macro avg      0.503     0.469     0.444        50\n",
      "weighted avg      0.526     0.440     0.446        50\n",
      "\n",
      "\n",
      "Errors for positive:\n",
      "Tweet: Time Wheel - Now It's an Indian to Look after British. #RishiSunakPM #RishiSunak #election #India #UKPrimeMinister #PrimeMinister, Gold: positive, VADER: neutral\n",
      "Tweet: Labour lawmaker Nadia Whittome:‚ÄúHe‚Äôs a multi-millionaire who, as chancellor, cut taxes on bank profits while overseeing the biggest drop in living standards since 1956. Black, white or Asian: if you work for a living, he is not on your side.‚Äù.#Britain..#Tories..#RishiSunak.., Gold: positive, VADER: negative\n",
      "Tweet: @JhaSanjay India is inspiring millions and several nations, except disgustingly fake narrative pushers. #RishiSunak is an inspired and proud Hindu, unlike the pseudo-secular frauds who pursue appeasement for votes and spread discrimination in society., Gold: positive, VADER: negative\n",
      "Tweet: Indians are not getting good teachers both in govt and private education institutions from KG to PG,even after spending money. We should learn from West how to teach systematically &amp; positively. If Indians get good education they will do wonders. #RishiSunak, Gold: positive, VADER: negative\n",
      "\n",
      "Errors for neutral:\n",
      "Tweet: @BritIndianVoice @RishiSunak #RishiSunak is #BritishHindu of paternal #Pakistani Punjabi origin. He‚Äôs legally POC (#Pakistan¬†Origin Card) holder issued by @NadraPak (probably also concurrently holding India‚Äôs OCI through his Indian NRI wife/mother). Also there may be issue with his wife‚Äôs Indian citizenship., Gold: neutral, VADER: positive\n",
      "Tweet: @trussliz @RishiSunak Chance was been given as PM but didn‚Äôt fulfilled her duty for people‚Ä¶ Not too late but #RishiSunak will take over and lets hope for the best for UK people üôè, Gold: neutral, VADER: positive\n",
      "Tweet: I see a lot of white ass is getting burnt... \n",
      "#RishiSunak, Gold: neutral, VADER: negative\n",
      "Tweet: ‚ÄúUnite or die‚Äù - strong lift from ol Ben Franklin there #rishisunak will you follow in his footsteps?, Gold: neutral, VADER: negative\n",
      "Tweet: #RishiSunak \n",
      "Love the way Pakistanis are claiming the new UK PM because his grandfather came from Gujranwala!\n",
      "(So did my father ü§™ü§™ü§™ü§™), Gold: neutral, VADER: positive\n",
      "Tweet: #RishiSunak\n",
      "\n",
      "Dear Indian left fake liberals - 500+ years Muslim invaders 200+ year the same British ruled India w‚Äôout people consultant \n",
      "\n",
      "Don‚Äôt give us lectures \n",
      "\n",
      "Even in independent India - So many minorities took height positions in india. PM, President, VP, CJI and Army Chief, Gold: neutral, VADER: negative\n",
      "Tweet: One thing is sure. \n",
      "\n",
      "Pakistan created by the party he is leading now.\n",
      "\n",
      "No change expect from him towards Pakistan.\n",
      " \n",
      "He will approve more aids to them in the name defense expense.\n",
      "\n",
      "He already told this in his speech.. regarding defense expense\n",
      "\n",
      "So head down Guys.\n",
      "\n",
      "#RishiSunak #UK, Gold: neutral, VADER: positive\n",
      "Tweet: Churchill in 1947: ‚ÄúIndians are not fit to rule, they are fit to be ruled.‚Äù\n",
      "\n",
      "Today, a person of Indian heritage was elected unopposed as the British PM. The Mayor of London has a Pakistani heritage.\n",
      "\n",
      "Maybe the colonizers forgot how to self-rule.\n",
      "\n",
      "#BritishPM #RishiSunak #SouthAsia, Gold: neutral, VADER: negative\n",
      "Tweet: I wonder if Britain's new Prime Minister is into Cos-Play?\n",
      "\n",
      "Lettuce hope not!\n",
      "\n",
      "#UKPM #UKPrimeMinister #britishpolitics #RishiSunak, Gold: neutral, VADER: positive\n",
      "Tweet: In another news, @akshaykumar signs a movie to play the role of #RishiSunak in his biography. \n",
      "Movie to be titled #NamasteyLondon2., Gold: neutral, VADER: positive\n",
      "\n",
      "Errors for negative:\n",
      "Tweet: #RishiSunak is the richest Tory MP in history. His wife is a billionaire. The suggestion that he is representative because he is Asian should sound ludicrous to your ears., Gold: negative, VADER: positive\n",
      "Tweet: A mountain full of money and a knowledge of proven theories with bit of hard work to use both can make the GORAS hug you and make a man the PM in just 5 years. All the best #RishiSunak @RubikaLiyaquat @anjanaomkashyap @sudhirchaudhary @irohitr @ARPITAARYA @nehabatham03 @J_Paatni, Gold: negative, VADER: positive\n",
      "Tweet: @RishiSunak has added a new meaning to the whole MBA to PM pipeline thing üôå#RishiSunak, Gold: negative, VADER: neutral\n",
      "Tweet: Now when hearing the name of UK feels like United Kailasa #UK #RishiSunak #UnitedKingdom  #GB #kailasa #BJP #HappyDiwali, Gold: negative, VADER: positive\n",
      "Tweet: Karma hits back #RishiSunak #indianrulingbritish, Gold: negative, VADER: neutral\n",
      "Tweet: I had a dream, which came true today, of a world where every British billionaire can grow up to become Prime Minister. #RishiSunak, Gold: negative, VADER: positive\n",
      "Tweet: Please, please keep reposting so that this oversight and blatant disregard is ever forgotten #rishisunak, Gold: negative, VADER: positive\n",
      "Tweet: Give assistance, not advise, in a crisis \n",
      "\n",
      "Good Morning üôà #Tuesday Treasure \n",
      "\n",
      "Do you have the urge to tell someone  to shut up even when they aren‚Äôt talking #RishiSunak #HappyDiwali #Superman #INDvsPAK #MAMA2022 #Quotidien #quote #Diwali #StrayKids #bajwa_traitor #BTSARMY #fun, Gold: negative, VADER: positive\n",
      "Tweet: @sunny_hundal We are not too much Happy,  this is the excitement of Indian news channels.\n",
      "@RishiSunak will not pay back us whatever Britishers looted from india .\n",
      "First of all , he is Englandian not Indian.\n",
      "Do you understand @sunny_hundal ?\n",
      "#England ,#EnglandPM , #English #RishiSunak ,, Gold: negative, VADER: positive\n",
      "Tweet: @LBC I question motives of someone with such great wealth to be able to identify with the general population of the UK #RishiSunak, Gold: negative, VADER: positive\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet)\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "\n",
    "report = classification_report(gold,all_vader_output,digits = 3)\n",
    "print(report)\n",
    "\n",
    "#error analysis\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Gold': gold,\n",
    "    'VADER': all_vader_output,\n",
    "    'Tweet': tweets\n",
    "})\n",
    "\n",
    "df['Error'] = df['Gold'] != df['VADER']\n",
    "\n",
    "errors = df[df['Error']]\n",
    "\n",
    "for sentiment in ['positive', 'neutral', 'negative']:\n",
    "    sentiment_errors = errors[errors['Gold'] == sentiment].head(10)\n",
    "    print(f\"\\nErrors for {sentiment}:\")\n",
    "    for _, row in sentiment_errors.iterrows():\n",
    "        print(f\"Tweet: {row['Tweet']}, Gold: {row['Gold']}, VADER: {row['VADER']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - part a (answer)\n",
    "Positive tweets have the lowest precision (0.308) but the highest recall (0.667), indicating the model often misclassifies tweets as positive but is good at catching most of the genuinely positive tweets. Neutral tweets exhibit higher precision (0.700) but lower recall (0.389), meaning while the model is relatively accurate when it predicts a tweet is neutral, it still misses many neutral tweets. For negative tweets, the precision is relatively low (0.500), indicating that only half of the tweets predicted as negative were correctly classified. The recall is even lower (0.350), suggesting the model misses a significant portion of truly negative tweets.\n",
    "\n",
    "Macro averages in this case are particularly relevant. They show VADERS performance across all sentiments, highlighting its strengths and weaknesses without bias toward more frequent categories. This balanced evaluation is essential for developing an unbiased understanding of public sentiment from social media data.\n",
    "\n",
    "Overall, macro precision and recall are about 50% showing it both misclassifies about hafl of the tweets and only catches about half of the true positive tweets.\n",
    "\n",
    "### Question 3 - part b (answer)\n",
    "\n",
    "Errors for positive:\n",
    "For these tweets even though the gold label is positive, VADER calculated them all as negative sentiment. This is likely because with perspective matters in these tweets. While they do convey some negative sentiment the message is positive towards Rishi Sunak. VADER only looks for general sentiment and is not aware that the outcome of these tweets is one that is positive towards Sunak and just registers the negative sentiment towards something else. \n",
    "\n",
    "Errors for neutral: \n",
    "This is admittedly a difficult label to know manually as these neutral tweets generally give both positive and negatives to outcome neutral. Also once again VADER is unaware that while a tweet can contain one sentiment it is overall a different sentiment towards the subject (Rishi Sunak). For example: Tweet: I see a lot of white ass is getting burnt... #RishiSunak (Gold: neutral, VADER: negative). While this tweet without context is of negative sentiment since it also contains #RishiSunak its sentiment is more complex and is neutral since it is the meaning of the sentence does not refer to Sunak but a statement of the settings around him. \n",
    "\n",
    "Errors for negative:\n",
    "VADER incorrectly labels most of these tweets as postitive sentiment when in fact they are negative. This happens multiple times with the mention of wealth. While VADER may measure 'wealth' and 'full of money' with positive sentiment in Sunak's case as a politician it is meant with negative sentiment due to the idea that this means he is out of touch with the British Population. Additionally, in other tweets, the user addresses the audience ('Good morning') which has strong positive sentiment but then goes goes on to express negative sentiment about Sunak which doesnt have as strong effect on VADER's overall sentiment calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---VADER AS IS---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.796     0.515     0.625      1750\n",
      "     neutral      0.604     0.507     0.551      1515\n",
      "    positive      0.558     0.881     0.684      1490\n",
      "\n",
      "    accuracy                          0.627      4755\n",
      "   macro avg      0.653     0.634     0.620      4755\n",
      "weighted avg      0.660     0.627     0.620      4755\n",
      "\n",
      "---VADER lemmatized---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.786     0.521     0.627      1750\n",
      "     neutral      0.598     0.489     0.538      1515\n",
      "    positive      0.556     0.879     0.681      1490\n",
      "\n",
      "    accuracy                          0.623      4755\n",
      "   macro avg      0.647     0.630     0.615      4755\n",
      "weighted avg      0.654     0.623     0.615      4755\n",
      "\n",
      "---VADER ADJ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.870     0.210     0.339      1750\n",
      "     neutral      0.403     0.893     0.555      1515\n",
      "    positive      0.666     0.436     0.527      1490\n",
      "\n",
      "    accuracy                          0.498      4755\n",
      "   macro avg      0.646     0.513     0.474      4755\n",
      "weighted avg      0.657     0.498     0.467      4755\n",
      "\n",
      "---VADER ADJ lemmatized---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.868     0.210     0.339      1750\n",
      "     neutral      0.403     0.893     0.556      1515\n",
      "    positive      0.665     0.436     0.526      1490\n",
      "\n",
      "    accuracy                          0.498      4755\n",
      "   macro avg      0.645     0.513     0.474      4755\n",
      "weighted avg      0.656     0.498     0.467      4755\n",
      "\n",
      "---VADER NOUN---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.728     0.143     0.240      1750\n",
      "     neutral      0.357     0.815     0.497      1515\n",
      "    positive      0.530     0.340     0.415      1490\n",
      "\n",
      "    accuracy                          0.419      4755\n",
      "   macro avg      0.538     0.433     0.384      4755\n",
      "weighted avg      0.548     0.419     0.376      4755\n",
      "\n",
      "---VADER NOUN lemmatized---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.714     0.157     0.257      1750\n",
      "     neutral      0.357     0.807     0.495      1515\n",
      "    positive      0.519     0.332     0.405      1490\n",
      "\n",
      "    accuracy                          0.419      4755\n",
      "   macro avg      0.530     0.432     0.386      4755\n",
      "weighted avg      0.539     0.419     0.379      4755\n",
      "\n",
      "---VADER VERB---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.776     0.289     0.421      1750\n",
      "     neutral      0.383     0.813     0.521      1515\n",
      "    positive      0.571     0.342     0.427      1490\n",
      "\n",
      "    accuracy                          0.472      4755\n",
      "   macro avg      0.577     0.481     0.456      4755\n",
      "weighted avg      0.586     0.472     0.455      4755\n",
      "\n",
      "---VADER VERB lemmatized---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.742     0.296     0.423      1750\n",
      "     neutral      0.378     0.783     0.509      1515\n",
      "    positive      0.570     0.350     0.434      1490\n",
      "\n",
      "    accuracy                          0.468      4755\n",
      "   macro avg      0.563     0.476     0.456      4755\n",
      "weighted avg      0.572     0.468     0.454      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import packages and load airline tweets\n",
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets_data = load_files(str(airline_tweets_folder))\n",
    "\n",
    "def index_to_label(list_index):\n",
    "    labels=[]\n",
    "    for i in list_index:\n",
    "        label=airline_tweets_data.target_names[i]\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "\n",
    "VADER_as_is=[]\n",
    "VADER_lemmatized=[]\n",
    "VADER_only_adjective=[]\n",
    "VADER_only_adjective_lemmatized=[]\n",
    "VADER_only_noun=[]\n",
    "VADER_only_noun_lemmatized=[]\n",
    "VADER_only_verb=[]\n",
    "VADER_only_verb_lemmatized=[]\n",
    "\n",
    "for tweet in airline_tweets_data.data:\n",
    "    tweet=str(tweet)[2:-1]\n",
    "    vader1=run_vader(tweet, lemmatize=False)\n",
    "    VADER_as_is.append(vader_output_to_label(vader1))\n",
    "\n",
    "    vader2=run_vader(tweet, lemmatize=True)\n",
    "    VADER_lemmatized.append(vader_output_to_label(vader2))\n",
    "\n",
    "    vader3=run_vader(tweet, lemmatize=False, parts_of_speech_to_consider='ADJ')\n",
    "    VADER_only_adjective.append(vader_output_to_label(vader3))\n",
    "\n",
    "    vader4=run_vader(tweet, lemmatize=True, parts_of_speech_to_consider='ADJ')\n",
    "    VADER_only_adjective_lemmatized.append(vader_output_to_label(vader4))\n",
    "\n",
    "    vader5=run_vader(tweet, lemmatize=False, parts_of_speech_to_consider='NOUN')\n",
    "    VADER_only_noun.append(vader_output_to_label(vader5))\n",
    "\n",
    "    vader6=run_vader(tweet, lemmatize=True, parts_of_speech_to_consider='NOUN')\n",
    "    VADER_only_noun_lemmatized.append(vader_output_to_label(vader6))\n",
    "\n",
    "    vader7=run_vader(tweet, lemmatize=False, parts_of_speech_to_consider='VERB')\n",
    "    VADER_only_verb.append(vader_output_to_label(vader7))\n",
    "\n",
    "    vader8=run_vader(tweet, lemmatize=True, parts_of_speech_to_consider='VERB')\n",
    "    VADER_only_verb_lemmatized.append(vader_output_to_label(vader8))\n",
    "   \n",
    "\n",
    "print('---VADER AS IS---\\n')\n",
    "report = classification_report(index_to_label(list(airline_tweets_data.target)),VADER_as_is,digits = 3)\n",
    "print(report)\n",
    "\n",
    "print('---VADER lemmatized---\\n')\n",
    "report = classification_report(index_to_label(list(airline_tweets_data.target)),VADER_lemmatized,digits = 3)\n",
    "print(report)\n",
    "\n",
    "print('---VADER ADJ---\\n')\n",
    "report = classification_report(index_to_label(list(airline_tweets_data.target)),VADER_only_adjective,digits = 3)\n",
    "print(report)\n",
    "\n",
    "print('---VADER ADJ lemmatized---\\n')\n",
    "report = classification_report(index_to_label(list(airline_tweets_data.target)),VADER_only_adjective_lemmatized,digits = 3)\n",
    "print(report)\n",
    "\n",
    "print('---VADER NOUN---\\n')\n",
    "report = classification_report(index_to_label(list(airline_tweets_data.target)),VADER_only_noun,digits = 3)\n",
    "print(report)\n",
    "\n",
    "print('---VADER NOUN lemmatized---\\n')\n",
    "report = classification_report(index_to_label(list(airline_tweets_data.target)),VADER_only_noun_lemmatized,digits = 3)\n",
    "print(report)\n",
    "\n",
    "print('---VADER VERB---\\n')\n",
    "report = classification_report(index_to_label(list(airline_tweets_data.target)),VADER_only_verb,digits = 3)\n",
    "print(report)\n",
    "\n",
    "print('---VADER VERB lemmatized---\\n')\n",
    "report = classification_report(index_to_label(list(airline_tweets_data.target)),VADER_only_verb_lemmatized,digits = 3)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sda1/envs/text-mining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---tfid min_df=2---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.784     0.947     0.858       337\n",
      "           1      0.880     0.720     0.792       307\n",
      "           2      0.891     0.850     0.870       307\n",
      "\n",
      "    accuracy                          0.842       951\n",
      "   macro avg      0.852     0.839     0.840       951\n",
      "weighted avg      0.850     0.842     0.840       951\n",
      "\n",
      "---counts min_df=2---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.838     0.974     0.901       351\n",
      "           1      0.953     0.782     0.859       312\n",
      "           2      0.909     0.906     0.908       288\n",
      "\n",
      "    accuracy                          0.891       951\n",
      "   macro avg      0.900     0.888     0.889       951\n",
      "weighted avg      0.897     0.891     0.889       951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sda1/envs/text-mining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---tfid min_df=4---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.856     0.905     0.879       367\n",
      "           1      0.824     0.728     0.773       302\n",
      "           2      0.818     0.858     0.837       282\n",
      "\n",
      "    accuracy                          0.835       951\n",
      "   macro avg      0.832     0.830     0.830       951\n",
      "weighted avg      0.834     0.835     0.833       951\n",
      "\n",
      "---counts min_df=4---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.880     0.939     0.908       359\n",
      "           1      0.868     0.800     0.833       280\n",
      "           2      0.890     0.885     0.887       312\n",
      "\n",
      "    accuracy                          0.880       951\n",
      "   macro avg      0.879     0.874     0.876       951\n",
      "weighted avg      0.880     0.880     0.879       951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sda1/envs/text-mining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---tfid min_df=6---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.849     0.911     0.879       359\n",
      "           1      0.833     0.738     0.782       290\n",
      "           2      0.835     0.854     0.845       302\n",
      "\n",
      "    accuracy                          0.840       951\n",
      "   macro avg      0.839     0.834     0.835       951\n",
      "weighted avg      0.840     0.840     0.839       951\n",
      "\n",
      "---counts min_df=6---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.829     0.949     0.885       352\n",
      "           1      0.926     0.778     0.845       306\n",
      "           2      0.873     0.867     0.870       293\n",
      "\n",
      "    accuracy                          0.869       951\n",
      "   macro avg      0.876     0.865     0.867       951\n",
      "weighted avg      0.874     0.869     0.868       951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sda1/envs/text-mining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---tfid min_df=8---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.824     0.902     0.862       338\n",
      "           1      0.847     0.728     0.783       320\n",
      "           2      0.820     0.857     0.838       293\n",
      "\n",
      "    accuracy                          0.830       951\n",
      "   macro avg      0.831     0.829     0.828       951\n",
      "weighted avg      0.831     0.830     0.828       951\n",
      "\n",
      "---counts min_df=8---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.834     0.931     0.880       362\n",
      "           1      0.888     0.762     0.820       302\n",
      "           2      0.854     0.857     0.856       287\n",
      "\n",
      "    accuracy                          0.855       951\n",
      "   macro avg      0.859     0.850     0.852       951\n",
      "weighted avg      0.857     0.855     0.854       951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sda1/envs/text-mining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---tfid min_df=10---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.826     0.905     0.864       357\n",
      "           1      0.846     0.747     0.793       293\n",
      "           2      0.837     0.837     0.837       301\n",
      "\n",
      "    accuracy                          0.835       951\n",
      "   macro avg      0.836     0.830     0.831       951\n",
      "weighted avg      0.836     0.835     0.834       951\n",
      "\n",
      "---counts min_df=10---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.847     0.951     0.896       349\n",
      "           1      0.869     0.784     0.824       296\n",
      "           2      0.894     0.853     0.873       306\n",
      "\n",
      "    accuracy                          0.868       951\n",
      "   macro avg      0.870     0.863     0.864       951\n",
      "weighted avg      0.869     0.868     0.866       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dfs=[2,4,6,8,10]\n",
    "\n",
    "for i in dfs:\n",
    "\n",
    "    airline_vec = CountVectorizer(min_df=i, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                                tokenizer=nltk.word_tokenize) \n",
    "    # stopwords are removedstop_words=stopwords.words('english')\n",
    "\n",
    "    airline_counts = airline_vec.fit_transform(airline_tweets_data.data)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "    docs_tfid_train, docs_tfid_test, y_tfid_train, y_tfid_test = train_test_split(\n",
    "        airline_tfidf, # the tf-idf model\n",
    "        airline_tweets_data.target, # the category values for each tweet \n",
    "        test_size = 0.20 # we use 80% for training and 20% for development\n",
    "        ) \n",
    "\n",
    "    docs_counts_train, docs_counts_test, y_counts_train, y_counts_test = train_test_split(\n",
    "        airline_tfidf, # the tf-idf model\n",
    "        airline_tweets_data.target, # the category values for each tweet \n",
    "        test_size = 0.20 # we use 80% for training and 20% for development\n",
    "        )\n",
    "\n",
    "    tfid_clf = MultinomialNB().fit(docs_tfid_train, y_tfid_train)\n",
    "    counts_clf = MultinomialNB().fit(docs_counts_train, y_counts_train)\n",
    "\n",
    "    tfid_y_pred = tfid_clf.predict(docs_tfid_test)\n",
    "    counts_y_pred = tfid_clf.predict(docs_counts_test)\n",
    "\n",
    "    print(f'---tfid min_df={i}---\\n')\n",
    "    report = classification_report(y_tfid_test,tfid_y_pred,digits = 3)\n",
    "    print(report)\n",
    "\n",
    "    print(f'---counts min_df={i}---\\n')\n",
    "    report = classification_report(y_counts_test,counts_y_pred,digits = 3)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sda1/envs/text-mining/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 162.07421750009578 united\n",
      "0 115.60386701619673 .\n",
      "0 99.99382694053722 @\n",
      "0 99.52395184333372 ``\n",
      "0 79.03888229479556 to\n",
      "0 68.93972736442352 i\n",
      "0 66.4869048130357 the\n",
      "0 59.762813193889905 a\n",
      "0 55.02131012131392 flight\n",
      "0 51.695025162695686 ?\n",
      "0 51.25141648392533 and\n",
      "0 49.5782399747483 is\n",
      "0 49.218693806843206 you\n",
      "0 48.0772867031744 my\n",
      "0 47.71408134110614 #\n",
      "0 45.62010143410105 on\n",
      "0 45.08385282660199 in\n",
      "0 43.22929023395862 !\n",
      "0 41.147325453645 for\n",
      "0 39.294184470831105 n't\n",
      "0 36.87981608737397 your\n",
      "0 36.14752801663445 no\n",
      "0 35.86999262639082 of\n",
      "0 35.5698861216441 it\n",
      "0 33.41387279718946 not\n",
      "0 33.05361901417877 that\n",
      "0 31.992341457899226 was\n",
      "0 30.282987482661365 have\n",
      "0 29.173781472250337 at\n",
      "0 28.418963180654718 with\n",
      "0 27.161488300587006 me\n",
      "0 26.128819653872206 this\n",
      "0 24.983083916676623 virginamerica\n",
      "0 24.96214101272257 ''\n",
      "0 24.13833871497802 service\n",
      "0 23.73377671639408 's\n",
      "0 22.3556782624379 delayed\n",
      "0 22.33461157883637 do\n",
      "0 22.15577418213356 be\n",
      "0 20.93067580317864 now\n",
      "0 20.480803734236417 we\n",
      "0 20.40180011191222 an\n",
      "0 20.352107824141473 plane\n",
      "0 20.303695251217373 cancelled\n",
      "0 20.242049383172933 are\n",
      "0 19.694133349505872 why\n",
      "0 19.55037468476278 bag\n",
      "0 19.455020993492397 customer\n",
      "0 19.23044331298898 'm\n",
      "0 19.032053914535805 get\n",
      "0 18.648102778460704 :\n",
      "0 18.553391882268073 been\n",
      "0 18.54937440602681 from\n",
      "0 17.904639480318185 just\n",
      "0 17.653365883367346 but\n",
      "0 17.34582658957722 time\n",
      "0 16.8572852211477 hours\n",
      "0 16.458190977425943 gate\n",
      "0 16.416692801635197 hour\n",
      "0 16.306348912563312 ...\n",
      "0 16.281774852568926 they\n",
      "0 16.116464731862074 -\n",
      "0 15.683060628611619 late\n",
      "0 15.677800466229895 worst\n",
      "0 15.60137802042353 what\n",
      "0 15.537580413179704 did\n",
      "0 15.456754799398468 up\n",
      "0 15.039912443360901 airline\n",
      "0 14.70745863701653 ;\n",
      "0 14.528431887673385 waiting\n",
      "0 14.349212999421443 http\n",
      "0 14.233158179809621 still\n",
      "0 13.974399540201055 &\n",
      "0 13.78231003375506 again\n",
      "0 13.470276709634819 after\n",
      "0 13.414827821327597 would\n",
      "0 13.379834972510322 2\n",
      "0 13.359764427127123 delay\n",
      "0 13.292937975942175 as\n",
      "0 13.24343537792216 one\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 114.83809532203186 @\n",
      "1 86.83693154009207 ?\n",
      "1 75.27591774949542 to\n",
      "1 68.65931704359143 jetblue\n",
      "1 65.01886463163312 southwestair\n",
      "1 61.95954068272796 i\n",
      "1 61.44699633968872 ``\n",
      "1 61.11168896791382 :\n",
      "1 58.58322485702673 .\n",
      "1 49.625056716278245 americanair\n",
      "1 47.589600887210246 #\n",
      "1 45.63715959858403 http\n",
      "1 43.416920387016035 a\n",
      "1 43.0179749626514 you\n",
      "1 41.56960603037443 usairways\n",
      "1 41.000193878199966 on\n",
      "1 40.44658390941889 united\n",
      "1 40.42408566534889 the\n",
      "1 39.938447606241134 flight\n",
      "1 35.326404923776096 can\n",
      "1 33.437231702665784 my\n",
      "1 31.536263130648713 for\n",
      "1 31.232023564502335 in\n",
      "1 31.12237773781164 is\n",
      "1 31.061788834584547 's\n",
      "1 30.441832690183336 from\n",
      "1 27.17770836699334 do\n",
      "1 25.62020921684528 !\n",
      "1 25.047647181945344 it\n",
      "1 24.92704539778529 have\n",
      "1 24.444458327621753 of\n",
      "1 23.75201897179674 virginamerica\n",
      "1 23.59315519183645 and\n",
      "1 22.522937751682193 me\n",
      "1 20.856484690523818 what\n",
      "1 19.9691117377387 -\n",
      "1 19.63103914550854 any\n",
      "1 19.482891407130246 please\n",
      "1 19.32518164693463 dm\n",
      "1 19.253761007223865 with\n",
      "1 18.95437201124292 get\n",
      "1 18.372787563674432 flights\n",
      "1 17.35341936618623 will\n",
      "1 17.148895790029076 are\n",
      "1 16.902971511064663 hi\n",
      "1 16.557911021305106 there\n",
      "1 16.46711424775204 (\n",
      "1 16.216772819470858 that\n",
      "1 16.05862607679456 our\n",
      "1 16.039898875823194 )\n",
      "1 15.95517841620213 if\n",
      "1 15.874871759325895 need\n",
      "1 15.874111758611644 at\n",
      "1 15.550485370301331 be\n",
      "1 15.547761045876431 how\n",
      "1 15.357364634694944 help\n",
      "1 15.201435158326081 this\n",
      "1 14.44873613646343 tomorrow\n",
      "1 14.36155569451969 when\n",
      "1 13.490180003934654 fleek\n",
      "1 13.357805830735428 sent\n",
      "1 13.350879118791422 fleet\n",
      "1 13.173940832369741 out\n",
      "1 13.045214949529857 now\n",
      "1 12.986958800121728 just\n",
      "1 12.762318515605324 ...\n",
      "1 12.457084421230586 ‚Äú\n",
      "1 12.210214247831919 we\n",
      "1 11.604184720152118 your\n",
      "1 11.541194200704107 ‚Äù\n",
      "1 11.496542059326073 would\n",
      "1 11.280465407391867 so\n",
      "1 10.402767412088307 about\n",
      "1 10.341805728419699 n't\n",
      "1 10.30728820183324 flying\n",
      "1 9.791307169325588 number\n",
      "1 9.782521513766204 or\n",
      "1 9.718895728082424 know\n",
      "1 9.698532108997044 change\n",
      "1 9.61328517839931 way\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 177.4382271309715 !\n",
      "2 114.01953499629053 @\n",
      "2 91.8407041212125 .\n",
      "2 87.5011467503879 you\n",
      "2 87.18461154323204 thanks\n",
      "2 80.45526836759997 thank\n",
      "2 74.0255245954563 southwestair\n",
      "2 71.28664874498921 ``\n",
      "2 70.27654875285089 jetblue\n",
      "2 60.40909645013111 the\n",
      "2 58.44581290543898 for\n",
      "2 56.940560756245944 #\n",
      "2 52.49653118543861 americanair\n",
      "2 50.22514439781771 to\n",
      "2 44.82757603445695 i\n",
      "2 42.04553237297085 great\n",
      "2 40.20971908660779 united\n",
      "2 37.06487544464817 :\n",
      "2 36.93139041025307 usairways\n",
      "2 33.3226478911754 a\n",
      "2 31.558328185985232 flight\n",
      "2 30.95928699122808 and\n",
      "2 28.566755631486735 my\n",
      "2 27.24288552937443 so\n",
      "2 26.81292643111622 your\n",
      "2 25.558873710078895 it\n",
      "2 25.532493532328033 love\n",
      "2 25.166669340394876 on\n",
      "2 24.490401360268482 virginamerica\n",
      "2 23.955725191755253 much\n",
      "2 23.057098651062464 service\n",
      "2 22.365460833178744 me\n",
      "2 22.117450778169424 )\n",
      "2 22.042607058732873 awesome\n",
      "2 21.768125838640007 with\n",
      "2 20.706878032735766 best\n",
      "2 20.6236927356384 of\n",
      "2 20.480884398114906 was\n",
      "2 20.244242802511568 in\n",
      "2 19.817719756069945 at\n",
      "2 19.69454831126565 is\n",
      "2 18.867488404876685 guys\n",
      "2 18.584992171050413 customer\n",
      "2 18.475713427557082 http\n",
      "2 17.907925662012165 good\n",
      "2 17.162109170627293 just\n",
      "2 16.584656497405327 this\n",
      "2 16.387852863498637 got\n",
      "2 16.186601122536274 are\n",
      "2 15.648011148104983 all\n",
      "2 15.562120212790045 amazing\n",
      "2 14.861387022266763 that\n",
      "2 14.70013897737122 -\n",
      "2 13.742332811728271 's\n",
      "2 13.459019409516953 airline\n",
      "2 13.358426969790184 very\n",
      "2 13.25856986755648 have\n",
      "2 12.902051459094073 ;\n",
      "2 12.657185765508808 be\n",
      "2 12.634736763143795 up\n",
      "2 12.491263912725685 time\n",
      "2 12.456061419169494 out\n",
      "2 12.40924546510625 appreciate\n",
      "2 11.796341072230364 today\n",
      "2 11.750773296013513 will\n",
      "2 11.694213298847542 from\n",
      "2 11.379886878023312 made\n",
      "2 11.316356357268381 response\n",
      "2 11.069239092919426 we\n",
      "2 10.494877904473194 flying\n",
      "2 10.347241985899547 us\n",
      "2 9.91494489486616 well\n",
      "2 9.911816749836152 help\n",
      "2 9.608044582090416 fly\n",
      "2 9.461536472739022 new\n",
      "2 9.135845577958293 again\n",
      "2 9.070341605094582 n't\n",
      "2 8.683677900760756 yes\n",
      "2 8.576058818932458 gate\n",
      "2 8.510852896519877 &\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=i, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                            tokenizer=nltk.word_tokenize) \n",
    "# stopwords are removedstop_words=stopwords.words('english')\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_data.data)\n",
    "\n",
    "docs_counts_train, docs_counts_test, y_counts_train, y_counts_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_data.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    )\n",
    "\n",
    "counts_clf = MultinomialNB().fit(docs_counts_train, y_counts_train)\n",
    "\n",
    "\n",
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "important_features_per_class(airline_vec, counts_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
