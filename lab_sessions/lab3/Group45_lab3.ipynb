{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question 1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Answer 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three scores of the VADER output are ratios for the proportion of text that falls into the categories of 'negative', 'neutral' or 'positive'. Together they should add up to one. To rate the sentiment of these individual parts of text, VADER utilizes a valence based approach which means that not only the positive or negative sentiment of a word is assessed but also the intensity of the sentiment is taken into account. VADER retrieves these sentiment scores from a lexicon containing a vast amount of words including slang terms and punctuation. Additionally, VADER captures contextual nuances such as capitalization, where capital letters intensify sentiment, and everything that comes after the word 'but' influences the sentiment of the text that came before it. Finally a 'compound' score is calculated by adding up all the lexicon ratings and standardizing them to a rating between -1 and 1 which stand for extremely negative and extremely positive respectively. With that in mind we will analyze the scores of the sentences given above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -SENTENCE 1: The positive sentiment for this sentence seems to be identified due to the word 'love,' which has a relatively high positive mean sentiment rating in the lexicon (3.2). No word in this sentence necessitates the use of negation or intensification rules.\n",
    "    \n",
    "    -SENTENCE 2: The negative sentiment for this sentence is due to the use of \"don't\" before \"love\" which triggers negation handling, switching the sentiment to negative compared to the first sentence, resulting in a negative compound score (-0.5216). No word in this sentence necessitates the intensification rule.\n",
    "\n",
    "    -SENTENCE 3: The addition of ':-)', which has a positive mean sentiment rating in the lexicon of 1.3, increases the positive sentiment compared to the first sentence. This is reflected in a higher positive score (0.867) and compound score (0.7579). The increase in positive scores is attributable to VADER's capacity to recognize and assign sentiment values to emoticons, in addition to words. No word in this sentence necessitates the use of the negation or intensification rules.\n",
    "\n",
    "    -SENTENCE 4: The word \"ruins\" in this sentence is interpreted negatively in VADER's lexicon with a negative mean sentiment rating of -1.9, leading to a negative sentiment. While \"ruins\" can have a negative connotation, in this context, it might not necessarily be negative (e.g the houses reffered to in this sentence could be historic buildings). This highlights a limitation in VADER's ability to understand context. No word in this sentence necessitates the use of the negation or intensification rules.\n",
    "\n",
    "    -SENTENCE 5: The negation \"not\" applied to the concept of 'ruins' and the intensifier \"certainly\" are handled here, making the sentiment positive. Upon inspection, \"considered\" does not appear to add any sentiment value, likely because it is not associated with any specific sentiment score in VADER's lexicon. The result seems somewhat misleading when considering the neutral nature of the statement. VADER correctly inverts the sentiment due to negation but might be overestimating the positive score (0.49) leading to a more positive compound score(0.5867). This again seems to highlight a limitation in VADER's ability to understand context, similar to sentence 4.\n",
    "\n",
    "    -SENTENCE 6: This sentence seems to be incorrectly identified as negative with a compound score of (-0.4215), likely because of the word \"lies\", which has a negative mean sentiment rating of -1.8. This seems to underscore a limitation in VADER's ability to understand context, in particularly with polysemous words. 'Lies' is interpreted as dishonesty, whereas, in this context, it simply means reclining. This highlights a flaw in VADER's classification ability for sentences containing words with multiple meanings. No word in this sentence necessitates the use of the negation or intensification rules.\n",
    "    \n",
    "    -SENTENCE 7: This sentence is identified with a positive sentiment, which may not entirely reflect the neutral nature of the comparison being made. The positive score (0.333) and compound score (0.3612) are unexpected, as the sentence appears neutral. This outcome may be attributed to the fact that 'like' is the only word from this sentence included in VADER's lexicon, carrying a positive mean sentiment rating of 1.5. This situation seems to highlight a limitation in VADER's ability to understand context, due to its limited lexicon. No word in this sentence necessitates the use of negation or intensification rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_tweets.json', encoding='utf-8') as f:\n",
    "    my_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'neutral', 'text_of_tweet': 'Poop Money android app is now available at tntdevelopment store. https://goo.gl/ddAn1E', 'tweet_url': 'https://twitter.com/RandomTweetsApp/status/856165981072306176'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "vader_model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "# use scikit-learn's classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet: Poop Money android app is now available at tntdevelopment store. https://goo.gl/ddAn1E\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: neutral\n",
      "\n",
      "The tweet: Haaland just surpassed Jesus season tally in one game\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: neutral\n",
      "\n",
      "The tweet: i want to corroborate the stories about the poor working conditions with wilbur and lovejoy. i have volunteered and worked for him and the band, and though i was spared any truly negative experiences, there was an immense lack of professionalism and care from bosses. that‚Äôs all.\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: ERLING HAALAND BRACE WITH ANOTHER DE BRUYNE ASSIST THE BEST DUO IN THE WORLD\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: Despite all negative news, this year will turn out , just as you prayed for ü§©ü§©‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è‚úîÔ∏è\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: Improve your mindset, no one likes to be around negative people.  #redo96\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Pochettino: ‚ÄúNo one says anything negative [about Liverpool or City]. It‚Äôs like if you win, you win, if you lose, you lose, it‚Äôs OK. Nothing happens. But in Chelsea it is completely different because of the pressure of that [¬£1billion spent on the squad]. For me, it is unfair, but in saying that, I accept the opinions.‚Äù\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Crossplay implemented into 1v1 and ranked but still not implemented into KING OF THE HILL despite ALL tournament organizers using KOTHS. Even casual players hop into KOTHS all the time. I don‚Äôt mean to be Mr. Negative here but you have to got to be trolling ü´†\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Jalen green under so he gets hit with the HardenUTD curse\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: the getty image curse got nothing on them\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: S01 E22 - The Curse of Buddy Buddy Temple\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: after what feels like forever, i've finally felt the motivation to do some vfx for fun again!!! üòç had this idea in my head for a few weeks, so just spent part of this evening making this!\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: Just for fun, i decided to do a voiceclaim comp for some of my comic's characters! Just what i'd imagine they'd sound like lol #furryart #doortophantasm\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: This is in syria , this poor girl loaded a tiktok video and then her JIHADISTS FAMILY did this to her , fuck jihadists ( not all Muslims or Arab are jihadists )  I hope israel wipes them out even by the use of unconventional weapons , hamas , hezbollah , houthis , iranian regime\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Commercial Sex Ring ‚Ä¶ ELECTED OFFICIALS INCLUDED IN THIS SEX RING‚Ä¶.. YOU ARE GOING DOWN YOU SICK FUCKS‚Ä¶ May want to pack your bags NOW!‚Ä¶..\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Fuck Hasan Piker All my homies hate Hasan Piker\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet:  fuck the government and fuck me\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: First win since November!!!! Fuck me that feels good. Forgotten what it feels like! And the fact we saw it out comfortably at the death as well. This team - Nathan Jones‚Äô RED ARMY #cafc\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: As a result of various bugs that we fixed in the most recent patch, a bug that allowed players to capture the tower boss was unintentionally fixed. We apologize for inadvertently fixing a bug.\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: He was eating somebody else‚Äôs leftovers but she took it away and gave him fresh food ü•∫\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: This could make a grown ass man cry\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: BREAKING: A woman who filmed herself killing a cat before putting the animal in a blender has been jailed for life for murdering a man four months later Read more: https://trib.al/Lz5lFvF üì∫ Sky 501, Virgin 602, Freeview 233 and YouTube\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: The dog played dead and saved the girl! This is why dogs are our best friends! Hero Dog! ‚ù§Ô∏è\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: Nintendo is suing one of the largest emulation projects for the Switch. Go fuck yourself Nintendo.\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: 'NOOOOOOOOO THEY'RE GONNA ANNOUNCE UNOVA REMAKES!! IT'S GONNA BE CHIBI STYLE!!!!! NOOOOOO' Kalos:\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: neutral\n",
      "\n",
      "The tweet: Danny Murphy whenever Blackburn are in possession #NUFC\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: neutral\n",
      "\n",
      "The tweet: Bro we cant score against blackburn, how were playing right now is genuinely worse than ashley era ü•≤\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: We're on the road once again, as we head down to Gloucestershire to take on Forest Green Rovers! üî¥‚ö™Ô∏è #WxmAFC\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: neutral\n",
      "\n",
      "The tweet: Mark Zuckerberg was in Japan making Katanas with a Japanese Sword Master\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: neutral\n",
      "\n",
      "The tweet: just got 110 burgers from Wendy's. nice try but your surge pricing won't get me, I'm stocked up for the year üñï\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: CAUSE MY HEART IS VERNON... VERNON... VERNOOOOOON\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: neutral\n",
      "\n",
      "The tweet: When stick is life.\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: neutral\n",
      "\n",
      "The tweet: No Five Guys tonight. I‚Äôm being the stereotypical black guy tonight. The gas station across the street even sells watermelon, too. ü§åüèæ\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Teachers Phone Gets Broken By Student Then Gets Physically Assaulted\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: The NATO is making moves to enrage Russia. This is going to be the reason behind WWIII. If Russia tries to fight NATO, it will get help from countries like China and North Korea. WWIII will be nothing like WWI and WWII. It will destroy civilization.\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Sukuna had 2 arms restrained by rika, one arm cut by yuta, one impaled by his sword, his tongue ripped out, his mouth cut open, jacob‚Äôs ladder attacking him from all angles and he somehow got a world cutting dismantle off\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Of course i ripped my pants at work now my balls are just outü§¶üèæ‚Äç‚ôÇÔ∏è\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: neutral\n",
      "\n",
      "The tweet: texting my boys kicking my feet giggling n shit & a lil fart slipped out\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Google to fix a problem they deliberately created. In other news BBC almost report an actual relevant story accurately‚Ä¶.almost.\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: The number of people playing Assassin's Creed 4: Black Flag has risen 200% on Steam after the release of Ubisoft's Skull and Bones. https://bit.ly/3TfSyms\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: People have started to cheat in Helldivers 2, a PVE game... Why? Like what are you cheating against, imagine being this boring üíÄ\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Haven't shot myself yet so far so good, you enjoying Rothschilds soul ?\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: If only I had made reading as pleasurable as masturbation. You all wouldn't be so stupid, and the world would be less crowded.\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: I got called an 'ugly motherfucker' by an old lady\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Woman pissed because boyfriend cooks gourmet meals 4 x a day and it‚Äôs his fault she‚Äôs fat LMAO üò≥üò≥üò≥\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: How is he still free??? This guy is evil as fuœ≤k he‚Äôs literally specifically targeting disabled people\n",
      "The gold label for this tweet: neutral\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Thanks, Satan.\n",
      "The gold label for this tweet: positive\n",
      "The VADER output for this tweet: positive\n",
      "\n",
      "The tweet: 'Why must I fail in every attempt at masonry?'\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: Kid knocks out another student outside the school  and leaves him unconscious and leaking‚Ä¶ #fightsvideos #fightvideos #fightingvideos #knockout #ko #shegotbeatup #Fights #fightvids #fightpage #fighting #FightVideo #fightsvideo #hoodfight #hoodfights #boyfight #girlfights #gore #death #schoolfights #hoodfights\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: negative\n",
      "\n",
      "The tweet: I really just want to do great work without having to ‚Äúperform‚Äù on social media. I‚Äôm getting less and less excited about ‚Äòpersonal branding.‚Äô\n",
      "The gold label for this tweet: negative\n",
      "The VADER output for this tweet: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tweets)):\n",
    "    print(f\"The tweet: {tweets[i]}\")\n",
    "    print(f\"The gold label for this tweet: {gold[i]}\")\n",
    "    print(f\"The VADER output for this tweet: {all_vader_output[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3a Perform a quantitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.82      0.62        17\n",
      "     neutral       0.44      0.27      0.33        15\n",
      "    positive       0.85      0.61      0.71        18\n",
      "\n",
      "    accuracy                           0.58        50\n",
      "   macro avg       0.60      0.57      0.56        50\n",
      "weighted avg       0.61      0.58      0.57        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "print(classification_report(gold, all_vader_output, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### precision is the proportion of correctly classified tweets for each sentiment category (negative, neutral, positive) compared to all tweets classified in that category:\n",
    "- precision(negative): 0.50 indicates that when the VADER classifies a tweet as negative, it is correct 50% of the time \n",
    "- precision(neutral): 0.44 suggests that when the VADER classifies a tweet as neutral, it is correct 44% of the time \n",
    "- precision(positive): 0.85 shows a high precision for positive classification. VADER is correct 85% of the time when it classifies a tweet is positive.\n",
    "\n",
    "#### recall is the proportion of actual tweets for each sentiment category (negative, neutral, positive) that were correctly identiified as such:\n",
    "- recall(negative): 0.82 shows a high recall for negative classification. This suggests that VADER is able to correctly identify 82% of all actual negative tweets.\n",
    "- recall(neutral): 0.27 indicates that VADER is able to correctly identify 27% of all actual neutral tweets.\n",
    "- recall(positive): 0.61 shows that the VADER correctly identifies 61% of all actual positive tweets\n",
    "\n",
    "#### f1-Score is the harmonic mean between precision and recall for each sentiment (where an improvement in precision can come at the cost of a decrease in recall, or vice versa):\n",
    "- f1-Score(negative): 0.62 suggest a relatively  balanced precision and recall for negative tweets.\n",
    "- f1-Score(neutral): 0.33 indicates a poor balance between precision and recall.\n",
    "- f1-Score(positive): 0.71 indicates a relatively better balance between precision and recall for positive tweets compared to neutral ones, but a relatively worse balance between precision and recall for the negative ones.\n",
    "\n",
    "#### support is the count of labelled tweets for each sentiment  (negative, neutral, positive), and can be used as an indication to assess class imbalance: \n",
    "- support(negative): there are 17 gold labeled negative tweets.\n",
    "- support(neutral): there are 15 gold labeled neutral tweets.\n",
    "- support(positive): there are 18 gold labeled positive tweets.\n",
    "\n",
    "#### accuracy is the proportion of correctly classified tweets  accross all classifications made, across all sentiments:\n",
    "- 0.58 indicates that VADER correctly identifies the sentiment of a tweet 58% of the time across all categories.\n",
    "\n",
    "#### macro avg: Averages the precision, recall, and f1-score per sentiment: negative, neutral, and positive, without considering the number of instances of each sentiment i.e. each sentiment is given equal weight:\n",
    "- The macro average precision is 0.60, recall is 0.57, and f1-score is 0.56.\n",
    "\n",
    "#### weighted avg: Averages the precision, recall, and F1-score per sentiment: negative, neutral, and positive, taking into account the number of true instances of each sentiment. This approach gives more weight to sentiments with more instances, making it a useful metric in imbalanced datasets:\n",
    "- The weighted average precision is 0.61, recall is 0.58, and f1-score is 0.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- VADER seems to perform best at classifying positive tweets, as indicated by the high precision and reasonably good recall and F1-score for positive sentiment.\n",
    "- VADER seems to struggles with neutral tweets the most, showing the lowest recall and F1-score, indicating difficulty in correctly identifying neutral sentiment accurately\n",
    "- Negative sentiment has a relatively high recall but lower precision, suggesting the model frequently predicts tweets as negative, even when they are not, but is quite good at capturing most negative instances.\n",
    "- An accuracy of 0.58 suggests moderate performance, with room for improvement, especially in correctly identifying neutral tweets and improving the precision for negative tweets.\n",
    "\n",
    "#### Most relevant scores:\n",
    "- The most relevant scores depend on the use case of a model. Here since neither the cost of false positive is high (precision) nor is it of utmost importance to capture as many true instances as possible (recall), the f1-score emerges as a particularly valuable metric. This is due to its balanced consideration of both precision and recall, offering a inclusive view of VADER's performance. Looking at the f1-scores across different sentiments can help identify which specific areas (positive, negative, neutral) might require improvements, such as adjusting the parameters of the 'run_vader' function to better capture neutral sentiments, in this case. The support metric indicates that hte sentiments seem to be fairly balanced, and since each sentiment category can be considered equally important, the macro avg for the f1-score is likely the single best metric to track in order to asses the performance of VADER in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3b Perform an error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Errors for positive:\n",
      "Tweet 1: Haaland just surpassed Jesus season tally in one game, Gold: positive, VADER: neutral\n",
      "Tweet 5: Improve your mindset, no one likes to be around negative people.  #redo96, Gold: positive, VADER: negative\n",
      "Tweet 6: Pochettino: ‚ÄúNo one says anything negative [about Liverpool or City]. It‚Äôs like if you win, you win, if you lose, you lose, it‚Äôs OK. Nothing happens. But in Chelsea it is completely different because of the pressure of that [¬£1billion spent on the squad]. For me, it is unfair, but in saying that, I accept the opinions.‚Äù, Gold: positive, VADER: negative\n",
      "Tweet 20: This could make a grown ass man cry, Gold: positive, VADER: negative\n",
      "Tweet 27: We're on the road once again, as we head down to Gloucestershire to take on Forest Green Rovers! üî¥‚ö™Ô∏è #WxmAFC, Gold: positive, VADER: neutral\n",
      "Tweet 31: When stick is life., Gold: positive, VADER: neutral\n",
      "Tweet 37: texting my boys kicking my feet giggling n shit & a lil fart slipped out, Gold: positive, VADER: negative\n",
      "\n",
      "Errors for neutral:\n",
      "Tweet 7: Crossplay implemented into 1v1 and ranked but still not implemented into KING OF THE HILL despite ALL tournament organizers using KOTHS. Even casual players hop into KOTHS all the time. I don‚Äôt mean to be Mr. Negative here but you have to got to be trolling ü´†, Gold: neutral, VADER: negative\n",
      "Tweet 9: the getty image curse got nothing on them, Gold: neutral, VADER: negative\n",
      "Tweet 10: S01 E22 - The Curse of Buddy Buddy Temple, Gold: neutral, VADER: negative\n",
      "Tweet 16:  fuck the government and fuck me, Gold: neutral, VADER: negative\n",
      "Tweet 18: As a result of various bugs that we fixed in the most recent patch, a bug that allowed players to capture the tower boss was unintentionally fixed. We apologize for inadvertently fixing a bug., Gold: neutral, VADER: positive\n",
      "Tweet 21: BREAKING: A woman who filmed herself killing a cat before putting the animal in a blender has been jailed for life for murdering a man four months later Read more: https://trib.al/Lz5lFvF üì∫ Sky 501, Virgin 602, Freeview 233 and YouTube, Gold: neutral, VADER: negative\n",
      "Tweet 32: No Five Guys tonight. I‚Äôm being the stereotypical black guy tonight. The gas station across the street even sells watermelon, too. ü§åüèæ, Gold: neutral, VADER: negative\n",
      "Tweet 35: Sukuna had 2 arms restrained by rika, one arm cut by yuta, one impaled by his sword, his tongue ripped out, his mouth cut open, jacob‚Äôs ladder attacking him from all angles and he somehow got a world cutting dismantle off, Gold: neutral, VADER: negative\n",
      "Tweet 38: Google to fix a problem they deliberately created. In other news BBC almost report an actual relevant story accurately‚Ä¶.almost., Gold: neutral, VADER: negative\n",
      "Tweet 42: If only I had made reading as pleasurable as masturbation. You all wouldn't be so stupid, and the world would be less crowded., Gold: neutral, VADER: negative\n",
      "\n",
      "Errors for negative:\n",
      "Tweet 24: 'NOOOOOOOOO THEY'RE GONNA ANNOUNCE UNOVA REMAKES!! IT'S GONNA BE CHIBI STYLE!!!!! NOOOOOO' Kalos:, Gold: negative, VADER: neutral\n",
      "Tweet 36: Of course i ripped my pants at work now my balls are just outü§¶üèæ‚Äç‚ôÇÔ∏è, Gold: negative, VADER: neutral\n",
      "Tweet 49: I really just want to do great work without having to ‚Äúperform‚Äù on social media. I‚Äôm getting less and less excited about ‚Äòpersonal branding.‚Äô, Gold: negative, VADER: positive\n"
     ]
    }
   ],
   "source": [
    "#error analysis\n",
    "df = pd.DataFrame({\n",
    "'Gold': gold,\n",
    "'VADER': all_vader_output,\n",
    "'Tweet': tweets\n",
    "})\n",
    "df['Error'] = df['Gold'] != df['VADER']\n",
    "errors = df[df['Error']]\n",
    "for sentiment in ['positive', 'neutral', 'negative']:\n",
    "    sentiment_errors = errors[errors['Gold'] == sentiment].head(10)\n",
    "    print(f\"\\nErrors for {sentiment}:\")\n",
    "    for _, row in sentiment_errors.iterrows():\n",
    "        print(f\"Tweet {_}: {row['Tweet']}, Gold: {row['Gold']}, VADER: {row['VADER']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis for positive:\n",
    "- Tweet 1: This tweet is likely classified as neutral because VADER doesn't recognize any of the words as positive sentiment-bearing words\n",
    "- Tweet 5: VADER fails to recognize the positive intention of the tweet. The word \"negative\" with a high mean sentiment rating of -2.7 and the phrase \"no one likes to be around\" likely overshadows the first part of the tweet and leads VADER to classify this tweet as negative. \n",
    "- Tweet 6: The relatively complex structure of this tweet contains both positive and negative aspects. The repeated use of words like \"negative\" and \"lose\" and discussion about unfair treatment likely leads VADER identifying the tweet as negative, as it makes up most of the tweet. VADER does not recognize the state of accaptance Pochettino declares (\"I accept the opinions\") as sufficient to identify the tweet as positive, due to their being less overall  positive sentiment-bearing words in the tweet.\n",
    "- Tweet 20: Is likely identified as negative due to the words\"ass\"(-2.5) and \"cry\"(-2.1). VADER does not seem capture the context in which crying is a positive response to a moving or heartwarming scenario. \n",
    "- Tweet 27: The excitement of the team's supporters are not captured by VADER due to a lack of positive sentiment-bearing words in the tweet\n",
    "- Tweet 31: The tweet is likely too vague, lacking sentiment-bearing words that VADER can recognize.\n",
    "- Tweet 37: The use of \"shit\" (-2.6) accompanied by a lack of positive sentiment-bearing words recognized by VADER, likely leads VADER to classify this tweet as negative. The overall humorous nature of the tweet is missed by VADER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis for neutral:\n",
    "- Tweet 7: Contains phrases that likely led to it being classified as negative by VADER, such as \"not implemented\" and \"I don‚Äôt mean to be Mr. Negative\", Despite the overall message being more of a critique than a purely negative sentiment\n",
    "- Tweet 9: The tweet is likely interpreted as negative due to the presence of \"curse\" which has a high negative mean sentiment rating in the lexicon of -2.5, despite the actual meaning being closer to neutral or quite possibly even positive (based on the context)\n",
    "- Tweet 10: Similar to the previous tweet, the word \"Curse\" in the title seemingly leads to a negative sentiment, despite the context being neutral (simply the title of an episode).\n",
    "- Tweet 16: The sarcastic nature of the tweet is not recognized by VADER. The use of \"fuck\" by the author directed towards the government and themselves is easily picked up as negative.\n",
    "- Tweet 18: Similar to the previous tweet, The sarcastic nature of this tweet is not recognized by VADER. This tweet's positive classification likely comes from words like \"allowed\", \"apologize\" that are included in the lexicon with a positive mean sentiment rating\n",
    "- Tweet 21:The negative sentiment detected by VADER is easily attributed to the explicit mention of violent and disturbing actions through words with high negative mean sentiment ratings e.g \"killing\", \"jailed\". Despite the news context, which may be considered neutral\n",
    "- Tweet 32: The use of words like \"no\" and \"stereotypical\" likely leads to the negative classification of the tweet, VADER again fails to recognize and handle the sarcastic nature of the tweet \n",
    "- Tweet 35: The negative sentiment detected by VADER can easily be attributed to the explicit mention of violent actions through words with negative mean sentiment ratings e.g \"cut\", \"attacking\". Despite the context(a description of a scene from an anime) which may be considered neutral\n",
    "- Tweet 38: VADER likely classifies this tweet as negative because of words with negative mean sentiment ratings like \"problem\" and dampening intensifiers like \"almost\". \n",
    "- Tweet 42: The presence of the negation \"wouldn't\" does not seem to be directly applied to \"stupid\"(-2.4), leading to VADER identifying the tweet as negative, without the ability to contextualize the humor or sarcasm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis for negative:\n",
    "- All three tweets seem to lack the use of common negation words that VADER could use to identify negative sentiment.\n",
    "- Tweet 24: The use of \"NOOOOOOOOO\" strongly indicates negative sentiment, which is not accounted for in VADER's lexicon or its rules\n",
    "- Tweet 36: Highlights VADER's inability to account for sarcasm, as indicated by the use of \"of course\" in the sentence. Additionally, VADER fails to capture the nuance of the ü§¶üèæ‚Äç‚ôÇÔ∏è emoticon, which is commonly used to indicate embarrassment\n",
    "- Tweet 49: Indicates an undesired situation the author of the tweet finds themselves in. \"Without having to\" should serve as a negation to the idea of performing on social media, but VADER might not fully capture the negative sentiment because the complex phrasing. The use of \"really\" and \"less and less excited\" indicate intensification but in a negative context. VADER might misinterpret \"really\" as positive reinforcement to \"great work\" leading to a positive sentiment that overshadows negative indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: C:\\Users\\remia\\Desktop\\period 4\\ba-text-mining\\lab_sessions\\lab3\\airlinetweets\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "print('path:', airline_tweets_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      airline_tweets_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\remia\\\\Desktop\\\\period 4\\\\ba-text-mining\\\\lab_sessions\\\\lab3\\\\airlinetweets'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(airline_tweets_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_tweets_all = load_files(str(airline_tweets_folder))\n",
    "#print(airline_tweets_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold labels for the airline tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_gold = []\n",
    "\n",
    "for i in range(len(airline_tweets_all.data)):\n",
    "    sentiment_index = airline_tweets_all.target[i]\n",
    "    airline_gold.append(airline_tweets_all.target_names[sentiment_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VADER (as it is) on the set of airline tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_tweets = []\n",
    "airline_vader_output = []\n",
    "\n",
    "for i in range(len(airline_tweets_all.data)):\n",
    "    # decoding bytes to string before run_vader (to deal with \"ExtraData: unpack(b) received extra data\" error)\n",
    "    if isinstance(airline_tweets_all.data[i], bytes):\n",
    "        textual_unit = airline_tweets_all.data[i].decode('utf-8')\n",
    "    else:\n",
    "        textual_unit = airline_tweets_all.data[i]\n",
    "    vader_output = run_vader(textual_unit) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    airline_tweets.append(textual_unit)\n",
    "    airline_vader_output.append(vader_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VADER on the set of airline tweets after having lemmatized the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_vader_output_lem = []\n",
    "\n",
    "for i in range(len(airline_tweets)):\n",
    "    textual_unit = airline_tweets[i]\n",
    "    vader_output = run_vader(textual_unit, lemmatize=True) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    airline_vader_output_lem.append(vader_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VADER on the set of airline tweets with only adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_vader_output_adj = []\n",
    "\n",
    "for i in range(len(airline_tweets_all.data)):\n",
    "    textual_unit = airline_tweets[i]\n",
    "    vader_output = run_vader(textual_unit, parts_of_speech_to_consider={'ADJ'}) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    airline_vader_output_adj.append(vader_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_vader_output_lem_adj = []\n",
    "\n",
    "for i in range(len(airline_tweets_all.data)):\n",
    "    textual_unit = airline_tweets[i]\n",
    "    vader_output = run_vader(textual_unit, lemmatize=True, parts_of_speech_to_consider={'ADJ'}) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    airline_vader_output_lem_adj.append(vader_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VADER on the set of airline tweets with only nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_vader_output_noun = []\n",
    "\n",
    "for i in range(len(airline_tweets_all.data)):\n",
    "    textual_unit = airline_tweets[i]\n",
    "    vader_output = run_vader(textual_unit, parts_of_speech_to_consider={'NOUN'}) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    airline_vader_output_noun.append(vader_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VADER on the set of airline tweets with only nouns and after having lemmatized the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_vader_output_lem_noun = []\n",
    "\n",
    "for i in range(len(airline_tweets_all.data)):\n",
    "    textual_unit = airline_tweets[i]\n",
    "    vader_output = run_vader(textual_unit, lemmatize=True, parts_of_speech_to_consider={'NOUN'}) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    airline_vader_output_lem_noun.append(vader_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VADER on the set of airline tweets with only verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_vader_output_verb = []\n",
    "\n",
    "for i in range(len(airline_tweets_all.data)):\n",
    "    textual_unit = airline_tweets[i]\n",
    "    vader_output = run_vader(textual_unit, parts_of_speech_to_consider={'VERB'}) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    airline_vader_output_verb.append(vader_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VADER on the set of airline tweets with only verbs and after having lemmatized the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_vader_output_lem_verb = []\n",
    "\n",
    "for i in range(len(airline_tweets_all.data)):\n",
    "    textual_unit = airline_tweets[i]\n",
    "    vader_output = run_vader(textual_unit, lemmatize=True, parts_of_speech_to_consider={'VERB'}) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    airline_vader_output_lem_verb.append(vader_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a Answer. Generate for all separate experiments the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run VADER (as it is) on the set of airline tweets\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.63      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.64      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Run VADER (as it is) on the set of airline tweets\\n\")\n",
    "print(classification_report(airline_gold, airline_vader_output, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run VADER on the set of airline tweets after having lemmatized the text\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.63      1750\n",
      "     neutral       0.60      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Run VADER on the set of airline tweets after having lemmatized the text\\n\")\n",
    "print(classification_report(airline_gold, airline_vader_output_lem, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run VADER on the set of airline tweets with only adjectives\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.21      0.34      1750\n",
      "     neutral       0.40      0.89      0.56      1515\n",
      "    positive       0.66      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.51      0.47      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Run VADER on the set of airline tweets with only adjectives\\n\")\n",
    "print(classification_report(airline_gold, airline_vader_output_adj, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.21      0.34      1750\n",
      "     neutral       0.40      0.89      0.56      1515\n",
      "    positive       0.66      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.51      0.47      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\\n\")\n",
    "print(classification_report(airline_gold, airline_vader_output_lem_adj, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run VADER on the set of airline tweets with only nouns\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.14      0.24      1750\n",
      "     neutral       0.36      0.82      0.50      1515\n",
      "    positive       0.53      0.34      0.41      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.54      0.43      0.38      4755\n",
      "weighted avg       0.55      0.42      0.38      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Run VADER on the set of airline tweets with only nouns\\n\")\n",
    "print(classification_report(airline_gold, airline_vader_output_noun, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.16      0.26      1750\n",
      "     neutral       0.36      0.81      0.50      1515\n",
      "    positive       0.52      0.33      0.40      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.53      0.43      0.39      4755\n",
      "weighted avg       0.54      0.42      0.38      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\\n\")\n",
    "print(classification_report(airline_gold, airline_vader_output_lem_noun, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run VADER on the set of airline tweets with only verbs\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.29      0.42      1750\n",
      "     neutral       0.38      0.81      0.52      1515\n",
      "    positive       0.57      0.34      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.58      0.48      0.46      4755\n",
      "weighted avg       0.59      0.47      0.45      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Run VADER on the set of airline tweets with only verbs\\n\")\n",
    "print(classification_report(airline_gold, airline_vader_output_verb, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.30      0.42      1750\n",
      "     neutral       0.38      0.78      0.51      1515\n",
      "    positive       0.57      0.35      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.56      0.48      0.46      4755\n",
      "weighted avg       0.57      0.47      0.45      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\\n\")\n",
    "print(classification_report(airline_gold, airline_vader_output_lem_verb, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b Answer. Compare the scores and explain what they tell you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$$$$$$$$   Orlando $$\n",
    "\n",
    "Does lemmatisation help?\n"
    ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: C:\\Users\\remia\\Desktop\\period 4\\ba-text-mining\\lab_sessions\\lab3\\airlinetweets\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "print('path:', airline_tweets_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      airline_tweets_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\remia\\\\Desktop\\\\period 4\\\\ba-text-mining\\\\lab_sessions\\\\lab3\\\\airlinetweets'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(airline_tweets_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_tweets_data = load_files(str(airline_tweets_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remia\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\remia\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---tfid min_df=2---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.814     0.904     0.856       343\n",
      "           1      0.868     0.689     0.768       296\n",
      "           2      0.836     0.897     0.866       312\n",
      "\n",
      "    accuracy                          0.835       951\n",
      "   macro avg      0.839     0.830     0.830       951\n",
      "weighted avg      0.838     0.835     0.832       951\n",
      "\n",
      "---counts min_df=2---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.933     0.880       343\n",
      "           1      0.872     0.693     0.772       296\n",
      "           2      0.849     0.904     0.876       312\n",
      "\n",
      "    accuracy                          0.849       951\n",
      "   macro avg      0.852     0.843     0.843       951\n",
      "weighted avg      0.851     0.849     0.845       951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remia\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\remia\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---tfid min_df=5---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.827     0.892     0.858       343\n",
      "           1      0.828     0.730     0.776       296\n",
      "           2      0.847     0.869     0.858       312\n",
      "\n",
      "    accuracy                          0.834       951\n",
      "   macro avg      0.834     0.830     0.831       951\n",
      "weighted avg      0.834     0.834     0.832       951\n",
      "\n",
      "---counts min_df=5---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.847     0.901     0.873       343\n",
      "           1      0.844     0.747     0.792       296\n",
      "           2      0.861     0.894     0.877       312\n",
      "\n",
      "    accuracy                          0.851       951\n",
      "   macro avg      0.850     0.847     0.847       951\n",
      "weighted avg      0.850     0.851     0.849       951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remia\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\remia\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---tfid min_df=10---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.818     0.854     0.836       343\n",
      "           1      0.770     0.736     0.753       296\n",
      "           2      0.835     0.830     0.833       312\n",
      "\n",
      "    accuracy                          0.810       951\n",
      "   macro avg      0.808     0.807     0.807       951\n",
      "weighted avg      0.809     0.810     0.809       951\n",
      "\n",
      "---counts min_df=10---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.840     0.872     0.856       343\n",
      "           1      0.794     0.753     0.773       296\n",
      "           2      0.860     0.865     0.863       312\n",
      "\n",
      "    accuracy                          0.833       951\n",
      "   macro avg      0.831     0.830     0.830       951\n",
      "weighted avg      0.832     0.833     0.832       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs=[2,5,10]\n",
    "\n",
    "for i in dfs:\n",
    "\n",
    "    airline_vec = CountVectorizer(min_df=i, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                                tokenizer=nltk.word_tokenize, stop_words=stopwords.words('english'))\n",
    "\n",
    "    airline_counts = airline_vec.fit_transform(airline_tweets_data.data)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "    docs_tfid_train, docs_tfid_test, y_tfid_train, y_tfid_test = train_test_split(\n",
    "        airline_tfidf, # the tf-idf model\n",
    "        airline_tweets_data.target, # the category values for each tweet \n",
    "        test_size = 0.20, # we use 80% for training and 20% for development\n",
    "        random_state=1\n",
    "        ) \n",
    "\n",
    "    docs_counts_train, docs_counts_test, y_counts_train, y_counts_test = train_test_split(\n",
    "        airline_counts, # the bag of words model\n",
    "        airline_tweets_data.target, # the category values for each tweet \n",
    "        test_size = 0.20,\n",
    "        random_state=1 # we use 80% for training and 20% for development\n",
    "        )\n",
    "\n",
    "    tfid_clf = MultinomialNB().fit(docs_tfid_train, y_tfid_train)\n",
    "    counts_clf = MultinomialNB().fit(docs_counts_train, y_counts_train)\n",
    "\n",
    "    tfid_y_pred = tfid_clf.predict(docs_tfid_test)\n",
    "    counts_y_pred = tfid_clf.predict(docs_counts_test)\n",
    "\n",
    "    print(f'---tfid min_df={i}---\\n')\n",
    "    report = classification_report(y_tfid_test,tfid_y_pred,digits = 3)\n",
    "    print(report)\n",
    "\n",
    "    print(f'---counts min_df={i}---\\n')\n",
    "    report = classification_report(y_counts_test,counts_y_pred,digits = 3)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Answer $$$ Orlando\n",
    "####### Look at the results of the experiments with the different settings and try to explain why they differ:\n",
    "\n",
    "###### which category performs best, is this the case for any setting? \n",
    "- best for TF-IDF representation: min_df=2\n",
    "- best for Bag of words representation: min_df=5 \n",
    "###### does the frequency threshold affect the scores? Why or why not according to you\n",
    "- yes, but not by much it would seem. The removal of stopwords with 'stop_words=stopwords.words('english')', and very common tokens (min_df), which are often not useful for training a classification model, means that the vocabulary is already focused on more meaningful words. Adjusting min_df primarily excludes less frequent terms, which may not add much predictive power beyond what the more frequent, informative tokens already provide. This suggests that key determinants of class membership are captured by the more frequent, informative tokens that remain after the threshold is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: C:\\Users\\remia\\Desktop\\period 4\\ba-text-mining\\lab_sessions\\lab3\\airlinetweets\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "print('path:', airline_tweets_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      airline_tweets_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\remia\\\\Desktop\\\\period 4\\\\ba-text-mining\\\\lab_sessions\\\\lab3\\\\airlinetweets'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(airline_tweets_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_tweets_data = load_files(str(airline_tweets_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remia\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\remia\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                                tokenizer=nltk.word_tokenize, stop_words=stopwords.words('english'))\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_data.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test; Bag of words representation ('airline_count'), min_df=2)\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the Bag of words model with min_df=2. airline_counts_two is defined at question 5\n",
    "    airline_tweets_data.target, # the category values for each tweet \n",
    "    test_size = 0.20, # we use 80% for training and 20% for development\n",
    "    random_state=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multimoda Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results, find macro recall\n",
    "y_pred = clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88       362\n",
      "           1       0.86      0.73      0.79       317\n",
      "           2       0.79      0.85      0.82       272\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.84      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6a.Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 1499.0 @\n",
      "0 1367.0 united\n",
      "0 1212.0 .\n",
      "0 412.0 ``\n",
      "0 406.0 ?\n",
      "0 404.0 flight\n",
      "0 386.0 !\n",
      "0 309.0 #\n",
      "0 219.0 n't\n",
      "0 154.0 ''\n",
      "0 129.0 's\n",
      "0 113.0 service\n",
      "0 107.0 :\n",
      "0 104.0 virginamerica\n",
      "0 99.0 get\n",
      "0 91.0 cancelled\n",
      "0 90.0 time\n",
      "0 89.0 customer\n",
      "0 87.0 delayed\n",
      "0 86.0 plane\n",
      "0 85.0 bag\n",
      "0 73.0 hours\n",
      "0 72.0 ...\n",
      "0 71.0 'm\n",
      "0 66.0 http\n",
      "0 66.0 ;\n",
      "0 66.0 -\n",
      "0 64.0 gate\n",
      "0 63.0 airline\n",
      "0 62.0 late\n",
      "0 62.0 help\n",
      "0 60.0 &\n",
      "0 59.0 still\n",
      "0 59.0 hour\n",
      "0 58.0 would\n",
      "0 56.0 ca\n",
      "0 54.0 amp\n",
      "0 53.0 2\n",
      "0 51.0 've\n",
      "0 50.0 worst\n",
      "0 50.0 waiting\n",
      "0 50.0 one\n",
      "0 50.0 never\n",
      "0 50.0 $\n",
      "0 48.0 flights\n",
      "0 48.0 (\n",
      "0 47.0 like\n",
      "0 47.0 delay\n",
      "0 45.0 )\n",
      "0 40.0 wait\n",
      "0 40.0 flightled\n",
      "0 40.0 due\n",
      "0 40.0 back\n",
      "0 39.0 luggage\n",
      "0 39.0 lost\n",
      "0 39.0 fly\n",
      "0 39.0 check\n",
      "0 38.0 seat\n",
      "0 38.0 really\n",
      "0 37.0 people\n",
      "0 36.0 us\n",
      "0 36.0 day\n",
      "0 35.0 ever\n",
      "0 35.0 another\n",
      "0 35.0 3\n",
      "0 34.0 crew\n",
      "0 33.0 trying\n",
      "0 33.0 hold\n",
      "0 33.0 bags\n",
      "0 33.0 baggage\n",
      "0 32.0 ticket\n",
      "0 32.0 got\n",
      "0 32.0 airport\n",
      "0 31.0 u\n",
      "0 31.0 thanks\n",
      "0 30.0 terrible\n",
      "0 30.0 problems\n",
      "0 29.0 seats\n",
      "0 29.0 last\n",
      "0 28.0 phone\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 1397.0 @\n",
      "1 519.0 ?\n",
      "1 503.0 .\n",
      "1 297.0 jetblue\n",
      "1 276.0 :\n",
      "1 270.0 southwestair\n",
      "1 250.0 united\n",
      "1 244.0 #\n",
      "1 241.0 ``\n",
      "1 237.0 flight\n",
      "1 187.0 americanair\n",
      "1 172.0 http\n",
      "1 160.0 usairways\n",
      "1 155.0 !\n",
      "1 133.0 's\n",
      "1 80.0 get\n",
      "1 76.0 virginamerica\n",
      "1 74.0 -\n",
      "1 67.0 flights\n",
      "1 66.0 ''\n",
      "1 64.0 please\n",
      "1 61.0 help\n",
      "1 57.0 )\n",
      "1 53.0 ;\n",
      "1 51.0 need\n",
      "1 48.0 n't\n",
      "1 48.0 ...\n",
      "1 45.0 (\n",
      "1 45.0 &\n",
      "1 40.0 us\n",
      "1 40.0 tomorrow\n",
      "1 39.0 would\n",
      "1 38.0 dm\n",
      "1 36.0 ‚Äú\n",
      "1 35.0 know\n",
      "1 34.0 thanks\n",
      "1 34.0 fleet\n",
      "1 34.0 fleek\n",
      "1 34.0 cancelled\n",
      "1 34.0 amp\n",
      "1 34.0 'm\n",
      "1 33.0 ‚Äù\n",
      "1 33.0 flying\n",
      "1 31.0 change\n",
      "1 30.0 one\n",
      "1 29.0 way\n",
      "1 29.0 number\n",
      "1 29.0 hi\n",
      "1 29.0 fly\n",
      "1 29.0 could\n",
      "1 27.0 airport\n",
      "1 26.0 new\n",
      "1 26.0 like\n",
      "1 25.0 today\n",
      "1 25.0 time\n",
      "1 24.0 travel\n",
      "1 23.0 ticket\n",
      "1 23.0 go\n",
      "1 23.0 destinationdragons\n",
      "1 20.0 tickets\n",
      "1 20.0 first\n",
      "1 20.0 back\n",
      "1 20.0 2\n",
      "1 19.0 weather\n",
      "1 19.0 want\n",
      "1 19.0 start\n",
      "1 19.0 sent\n",
      "1 19.0 see\n",
      "1 19.0 make\n",
      "1 19.0 going\n",
      "1 19.0 follow\n",
      "1 19.0 chance\n",
      "1 19.0 booked\n",
      "1 18.0 trying\n",
      "1 18.0 question\n",
      "1 18.0 left\n",
      "1 18.0 guys\n",
      "1 18.0 dfw\n",
      "1 18.0 check\n",
      "1 18.0 add\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 1362.0 @\n",
      "2 1093.0 !\n",
      "2 781.0 .\n",
      "2 307.0 #\n",
      "2 304.0 thanks\n",
      "2 303.0 southwestair\n",
      "2 295.0 jetblue\n",
      "2 258.0 thank\n",
      "2 255.0 united\n",
      "2 236.0 ``\n",
      "2 190.0 flight\n",
      "2 184.0 :\n",
      "2 183.0 americanair\n",
      "2 137.0 usairways\n",
      "2 129.0 great\n",
      "2 94.0 )\n",
      "2 92.0 virginamerica\n",
      "2 89.0 service\n",
      "2 80.0 http\n",
      "2 76.0 best\n",
      "2 73.0 much\n",
      "2 73.0 love\n",
      "2 73.0 guys\n",
      "2 66.0 customer\n",
      "2 64.0 's\n",
      "2 59.0 ;\n",
      "2 54.0 -\n",
      "2 53.0 awesome\n",
      "2 51.0 good\n",
      "2 47.0 amazing\n",
      "2 47.0 airline\n",
      "2 46.0 got\n",
      "2 45.0 time\n",
      "2 42.0 &\n",
      "2 40.0 us\n",
      "2 40.0 today\n",
      "2 39.0 n't\n",
      "2 38.0 get\n",
      "2 36.0 crew\n",
      "2 35.0 made\n",
      "2 35.0 gate\n",
      "2 35.0 flying\n",
      "2 35.0 fly\n",
      "2 35.0 amp\n",
      "2 34.0 help\n",
      "2 33.0 appreciate\n",
      "2 33.0 ...\n",
      "2 31.0 response\n",
      "2 30.0 ever\n",
      "2 29.0 home\n",
      "2 29.0 back\n",
      "2 29.0 'm\n",
      "2 27.0 work\n",
      "2 27.0 u\n",
      "2 26.0 ?\n",
      "2 26.0 're\n",
      "2 25.0 see\n",
      "2 25.0 first\n",
      "2 25.0 day\n",
      "2 24.0 know\n",
      "2 23.0 tonight\n",
      "2 23.0 plane\n",
      "2 23.0 new\n",
      "2 23.0 flights\n",
      "2 23.0 (\n",
      "2 23.0 'll\n",
      "2 22.0 like\n",
      "2 22.0 helpful\n",
      "2 21.0 well\n",
      "2 21.0 staff\n",
      "2 21.0 please\n",
      "2 21.0 job\n",
      "2 21.0 ''\n",
      "2 20.0 yes\n",
      "2 20.0 really\n",
      "2 20.0 quick\n",
      "2 20.0 nice\n",
      "2 19.0 would\n",
      "2 19.0 team\n",
      "2 19.0 happy\n"
     ]
    }
   ],
   "source": [
    "# example of how to call from notebook:\n",
    "important_features_per_class(airline_vec, clf) # airline_vec_two defined at question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6b1.Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$$ michael"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
