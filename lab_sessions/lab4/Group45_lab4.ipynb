{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('/mnt/sda1/Text_Mining_Group45/lab_sessions/lab4/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "      'bias': 1.0,\n",
    "      'words': token,  # original token\n",
    "      'pos': pos,  # Part Of Speech tag of the token\n",
    "      'word.lower()': token.lower(),  # lower case variant of the token\n",
    "      'word[-3:]': token[-3:],  # suffix of 3 characters\n",
    "      'word[-2:]': token[-2:],  # suffix of 2 characters\n",
    "      'word.isupper()': token.isupper(),  # is the token in uppercase\n",
    "      'word.istitle()': token.istitle(),  # does the token start with a capital letter\n",
    "      'word.isdigit()': token.isdigit(),  # is the token a digit\n",
    "      'postag': pos,  # Part Of Speech tag\n",
    "      'postag[:2]': pos[:2],  # first two characters of the PoS tag\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader('/mnt/sda1/Text_Mining_Group45/lab_sessions/lab4/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "      'bias': 1.0,\n",
    "      'words': token,  # original token\n",
    "      'pos': pos,  # Part Of Speech tag of the token\n",
    "      'word.lower()': token.lower(),  # lower case variant of the token\n",
    "      'word[-3:]': token[-3:],  # suffix of 3 characters\n",
    "      'word[-2:]': token[-2:],  # suffix of 2 characters\n",
    "      'word.isupper()': token.isupper(),  # is the token in uppercase\n",
    "      'word.istitle()': token.istitle(),  # does the token start with a capital letter\n",
    "      'word.isdigit()': token.isdigit(),  # is the token a digit\n",
    "      'postag': pos,  # Part Of Speech tag\n",
    "      'postag[:2]': pos[:2],  # first two characters of the PoS tag\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of instances in training data: 203621\n",
      "Number of instances in test data: 46435\n",
      "Proportion test data: 0.1856984035576031\n",
      "\n",
      "\n",
      "Train NERC labels\n",
      "B-ORG: 6321 (3.1%)\n",
      "O: 169578 (83.28%)\n",
      "B-MISC: 3438 (1.69%)\n",
      "B-PER: 6600 (3.24%)\n",
      "I-PER: 4528 (2.22%)\n",
      "B-LOC: 7140 (3.51%)\n",
      "I-ORG: 3704 (1.82%)\n",
      "I-MISC: 1155 (0.57%)\n",
      "I-LOC: 1157 (0.57%)\n",
      "\n",
      "Test NERC labels\n",
      "O: 38323 (82.53%)\n",
      "B-LOC: 1668 (3.59%)\n",
      "B-PER: 1617 (3.48%)\n",
      "I-PER: 1156 (2.49%)\n",
      "I-LOC: 257 (0.55%)\n",
      "B-MISC: 702 (1.51%)\n",
      "I-MISC: 216 (0.47%)\n",
      "B-ORG: 1661 (3.58%)\n",
      "I-ORG: 835 (1.8%)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "count_train=Counter(training_gold_labels)\n",
    "count_test=Counter(test_gold_labels)\n",
    "\n",
    "#no of instances\n",
    "print(\"\\nNumber of instances in training data:\", len(training_gold_labels))\n",
    "print(\"Number of instances in test data:\", len(test_gold_labels))\n",
    "print(\"Proportion test data:\", len(test_gold_labels)/(len(test_gold_labels)+len(training_gold_labels)))\n",
    "print('\\n')\n",
    "# new_dict={}\n",
    "# #frequency distribution of NERC labels\n",
    "print('Train NERC labels')\n",
    "for label, freq in count_train.items():\n",
    "    print(f\"{label}: {freq} ({round((freq/len(training_gold_labels))*100,2)}%)\")\n",
    "\n",
    "print('\\nTest NERC labels')\n",
    "for label, freq in count_test.items():\n",
    "    print(f\"{label}: {freq} ({round((freq/len(test_gold_labels))*100, 2)}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (b)\n",
    "## Orlando TO DO (with micheals text)\n",
    "\n",
    "The frequency distribution of NERC labels in both the training and test datasets reveals a significant imbalance, with the 'O' (Outside of named entities) label dominating at 83.28% and 82.53% respectively. This skew towards non-entity tokens is common in NER tasks, reflecting the nature of natural language where named entities constitute a smaller portion of the text. Other common labels like 'B-LOC' (Beginning of a location), 'B-PER' (Beginning of a person's name), and 'I-PER' (Inside a person's name) show somewhat proportional representations between the training and test sets, with 'B-LOC' at 3.51% and 3.59%, 'B-PER' at 3.24% and 3.48%, and 'I-PER' at 2.22% and 2.49%, respectively. However, these entities, alongside 'B-ORG' (Beginning of an organization's name) and 'I-ORG' (Inside an organization's name), still make up a small fraction compared to the 'O' label, indicating a lack of balance in terms of named entity coverage.\n",
    "\n",
    "While there are similarities in the proportions of NERC labels between the training and test data, indicating a degree of consistency in data annotation and representation, differences are also evident. Specifically, minor entities such as 'I-MISC' (Inside a miscellaneous entity) and 'I-LOC' exhibit nearly identical proportions (around 0.57% in training and slightly less in test data), emphasizing the challenge in achieving a balanced dataset for these less frequent categories. The minor discrepancies in distribution percentages point to slight variations in entity representation, which could impact model performance, especially on the less represented categories. This imbalance suggests that while models trained on this data may perform well on identifying the 'O' label and somewhat well on more common entities like locations and persons, they might struggle with less frequent entities due to their underrepresentation in the training material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "features=training_features + test_features\n",
    "the_array = vec.fit_transform(features)\n",
    "# print(the_array)\n",
    "# print(len(the_array))\n",
    "\n",
    "n_train = len(training_features)\n",
    "\n",
    "train_array = the_array[:n_train]\n",
    "test_array = the_array[n_train:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.73      0.81      0.77      1668\n",
      "      B-MISC       0.71      0.70      0.71       702\n",
      "       B-ORG       0.68      0.58      0.63      1661\n",
      "       B-PER       0.68      0.59      0.63      1617\n",
      "       I-LOC       0.59      0.54      0.56       257\n",
      "      I-MISC       0.56      0.59      0.58       216\n",
      "       I-ORG       0.54      0.49      0.51       835\n",
      "       I-PER       0.48      0.54      0.51      1156\n",
      "           O       0.98      0.99      0.99     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.66      0.65      0.65     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "lin_clf.fit(train_array, training_gold_labels)\n",
    "test_predictions = lin_clf.predict(test_array)\n",
    "report = classification_report(test_gold_labels, test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer (d)\n",
    "## Micheal TO DO\n",
    "The classifier demonstrates strong performance particularly on the 'O' (Outside of named entities) label, achieving high precision, recall, and F1-score of 0.98, 0.99, and 0.99 respectively. This label, which represents tokens not classified as named entities, benefits from its overwhelming presence in the dataset, as evidenced by the provided data distribution. Such a dominant representation facilitates the classifier's learning, making it adept at identifying non-entity components of the text with high accuracy. The high performance on 'O' significantly contributes to the overall accuracy metric of 0.93, indicating that the classifier is particularly efficient at discerning non-entity text portions, likely due to the abundance of examples during training that bolster its predictive confidence and reduce the likelihood of false positives or negatives within this category.\n",
    "\n",
    "Conversely, the classifier shows comparatively weaker performance on several entity categories, notably 'I-PER' (Inside a person's name), 'I-ORG' (Inside an organization's name), and 'I-LOC' (Inside a location), with F1-scores of 0.51, 0.51, and 0.56 respectively. These categories are characterized by lower precision and recall, suggesting difficulties in accurately identifying and classifying tokens that are part of named entities extending beyond a single token. The relatively poor performance on these labels can be attributed to several factors, including the inherent complexity of recognizing entities that span multiple tokens, potential inconsistencies in labeling, and the relatively smaller number of examples for these categories compared to the 'O' label. This results in less training data for these specific entity types, complicating the model's task of learning their distinctive features amidst the linguistic variability of natural language. Additionally, the precision-recall trade-off observed, especially in 'I-PER' and 'I-ORG', points to challenges in balancing the detection of true positives against the avoidance of false positives, further complicating the accurate classification of these entity types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.80      0.78      1668\n",
      "      B-MISC       0.72      0.70      0.71       702\n",
      "       B-ORG       0.69      0.64      0.66      1661\n",
      "       B-PER       0.75      0.67      0.71      1617\n",
      "       I-LOC       0.51      0.42      0.46       257\n",
      "      I-MISC       0.60      0.54      0.57       216\n",
      "       I-ORG       0.48      0.33      0.39       835\n",
      "       I-PER       0.59      0.50      0.54      1156\n",
      "           O       0.97      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.62      0.64     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "##### Adapt the path to point to your local copy of the Google embeddings model\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/mnt/sda1/Text_Mining_Group45/lab_sessions/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "training_vectors=[]\n",
    "train_labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        training_vectors.append(vector)\n",
    "        train_labels.append(ne_label)\n",
    "\n",
    "test_vectors=[]\n",
    "test_labels=[]\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        test_vectors.append(vector)\n",
    "        test_labels.append(ne_label)\n",
    "\n",
    "lin_clf2 = svm.LinearSVC()\n",
    "lin_clf2.fit(training_vectors, train_labels)\n",
    "pred=lin_clf2.predict(test_vectors)\n",
    "\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remi TO DO\n",
    "\n",
    "The comparison between the classification results using manually crafted feature vectors and those obtained from embedding-based features shows a notable improvement in the model's performance when leveraging word embeddings. Specifically, for entity types such as B-LOC, B-MISC, B-ORG, and B-PER, there is a marked increase in precision and recall, leading to higher F1-scores. This improvement underscores the power of embeddings to capture semantic and contextual nuances of words, enhancing the model's ability to distinguish between different entity types more effectively. While the accuracy for non-entity type 'O' remains high in both cases, the use of embeddings has contributed to a more balanced performance across all entity types, as evidenced by the increase in macro-average F1-score from 0.65 to 0.74. This highlights the embeddings' role in providing a richer representation of tokens, facilitating better generalization and more nuanced entity recognition compared to traditional feature engineering approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = '/mnt/sda1/Text_Mining_Group45/lab_sessions/lab4/kaggle/ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050795"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16896/2272784935.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['sentence_idx']=new\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>...</th>\n",
       "      <th>prev-prev-lemma</th>\n",
       "      <th>prev-prev-pos</th>\n",
       "      <th>prev-prev-shape</th>\n",
       "      <th>prev-prev-word</th>\n",
       "      <th>prev-shape</th>\n",
       "      <th>prev-word</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>shape</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thousand</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>...</td>\n",
       "      <td>__start2__</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>...</td>\n",
       "      <td>__start1__</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>...</td>\n",
       "      <td>thousand</td>\n",
       "      <td>NNS</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>...</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     lemma next-lemma next-next-lemma next-next-pos next-next-shape  \\\n",
       "0   0  thousand         of        demonstr           NNS       lowercase   \n",
       "1   1        of   demonstr            have           VBP       lowercase   \n",
       "2   2  demonstr       have           march           VBN       lowercase   \n",
       "3   3      have      march         through            IN       lowercase   \n",
       "4   4     march    through          london           NNP     capitalized   \n",
       "\n",
       "  next-next-word next-pos next-shape      next-word  ... prev-prev-lemma  \\\n",
       "0  demonstrators       IN  lowercase             of  ...      __start2__   \n",
       "1           have      NNS  lowercase  demonstrators  ...      __start1__   \n",
       "2        marched      VBP  lowercase           have  ...        thousand   \n",
       "3        through      VBN  lowercase        marched  ...              of   \n",
       "4         London       IN  lowercase        through  ...        demonstr   \n",
       "\n",
       "  prev-prev-pos prev-prev-shape prev-prev-word   prev-shape      prev-word  \\\n",
       "0    __START2__        wildcard     __START2__     wildcard     __START1__   \n",
       "1    __START1__        wildcard     __START1__  capitalized      Thousands   \n",
       "2           NNS     capitalized      Thousands    lowercase             of   \n",
       "3            IN       lowercase             of    lowercase  demonstrators   \n",
       "4           NNS       lowercase  demonstrators    lowercase           have   \n",
       "\n",
       "  sentence_idx        shape           word tag  \n",
       "0          1.0  capitalized      Thousands   O  \n",
       "1          1.0    lowercase             of   O  \n",
       "2          1.0    lowercase  demonstrators   O  \n",
       "3          1.0    lowercase           have   O  \n",
       "4          1.0    lowercase        marched   O  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "\n",
    "sentence_column=df_train['sentence_idx']\n",
    "\n",
    "def string(x):\n",
    "    return (str(x))\n",
    "\n",
    "new=sentence_column.apply(string)\n",
    "df_train['sentence_idx']=new\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split df into labels and features arrays \n",
    "\n",
    "train_labels = df_train['tag'].values\n",
    "features_train = df_train.drop('tag', axis=1)\n",
    "features_train = features_train.drop('id', axis=1)\n",
    "# features_train = features_train.drop('sentence_idx', axis=1)\n",
    "features_dict_train = features_train.to_dict(orient='records')\n",
    "\n",
    "test_labels = df_test['tag'].values\n",
    "features_test = df_test.drop('tag', axis=1)\n",
    "features_test = features_test.drop('id', axis=1)\n",
    "# features_test = features_test.drop('sentence_idx', axis=1)\n",
    "features_dict_test = features_test.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn import svm\n",
    "# from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemma': 'of', 'next-lemma': 'demonstr', 'next-next-lemma': 'have', 'next-next-pos': 'VBP', 'next-next-shape': 'lowercase', 'next-next-word': 'have', 'next-pos': 'NNS', 'next-shape': 'lowercase', 'next-word': 'demonstrators', 'pos': 'IN', 'prev-iob': 'O', 'prev-lemma': 'thousand', 'prev-pos': 'NNS', 'prev-prev-iob': '__START1__', 'prev-prev-lemma': '__start1__', 'prev-prev-pos': '__START1__', 'prev-prev-shape': 'wildcard', 'prev-prev-word': '__START1__', 'prev-shape': 'capitalized', 'prev-word': 'Thousands', 'sentence_idx': '1.0', 'shape': 'lowercase', 'word': 'of'}\n",
      "  (0, 7329)\t1.0\n",
      "  (0, 13352)\t1.0\n",
      "  (0, 18437)\t1.0\n",
      "  (0, 24093)\t1.0\n",
      "  (0, 24122)\t1.0\n",
      "  (0, 29674)\t1.0\n",
      "  (0, 35348)\t1.0\n",
      "  (0, 35386)\t1.0\n",
      "  (0, 43763)\t1.0\n",
      "  (0, 46843)\t1.0\n",
      "  (0, 46883)\t1.0\n",
      "  (0, 47411)\t1.0\n",
      "  (0, 55054)\t1.0\n",
      "  (0, 55074)\t1.0\n",
      "  (0, 55577)\t1.0\n",
      "  (0, 62960)\t1.0\n",
      "  (0, 62973)\t1.0\n",
      "  (0, 67110)\t1.0\n",
      "  (0, 74411)\t1.0\n",
      "  (0, 78696)\t1.0\n",
      "  (0, 86270)\t1.0\n",
      "  (0, 90816)\t1.0\n",
      "  (0, 94729)\t1.0\n",
      "  (1, 5280)\t1.0\n",
      "  (1, 10446)\t1.0\n",
      "  :\t:\n",
      "  (119998, 90819)\t1.0\n",
      "  (119998, 99619)\t1.0\n",
      "  (119999, 4721)\t1.0\n",
      "  (119999, 11904)\t1.0\n",
      "  (119999, 21330)\t1.0\n",
      "  (119999, 24081)\t1.0\n",
      "  (119999, 24122)\t1.0\n",
      "  (119999, 32344)\t1.0\n",
      "  (119999, 35371)\t1.0\n",
      "  (119999, 35386)\t1.0\n",
      "  (119999, 42679)\t1.0\n",
      "  (119999, 46843)\t1.0\n",
      "  (119999, 46882)\t1.0\n",
      "  (119999, 52165)\t1.0\n",
      "  (119999, 55023)\t1.0\n",
      "  (119999, 55072)\t1.0\n",
      "  (119999, 60311)\t1.0\n",
      "  (119999, 62934)\t1.0\n",
      "  (119999, 62967)\t1.0\n",
      "  (119999, 71620)\t1.0\n",
      "  (119999, 74405)\t1.0\n",
      "  (119999, 83207)\t1.0\n",
      "  (119999, 86269)\t5469.0\n",
      "  (119999, 90819)\t1.0\n",
      "  (119999, 99202)\t1.0\n",
      "(100000, 102681)\n",
      "(20000, 102681)\n"
     ]
    }
   ],
   "source": [
    "vec2 = DictVectorizer()\n",
    "features2=features_dict_train + features_dict_test\n",
    "print(features2[1])\n",
    "the_array = vec2.fit_transform(features2)\n",
    "print(the_array)\n",
    "\n",
    "n_train = len(features_dict_train)\n",
    "\n",
    "train_array = the_array[:n_train]\n",
    "test_array = the_array[n_train:]\n",
    "\n",
    "print(train_array.shape)\n",
    "print(test_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         4\n",
      "       B-eve       0.00      0.00      0.00         0\n",
      "       B-geo       0.87      0.88      0.87       741\n",
      "       B-gpe       0.90      0.93      0.92       296\n",
      "       B-nat       0.80      0.50      0.62         8\n",
      "       B-org       0.77      0.67      0.72       397\n",
      "       B-per       0.81      0.83      0.82       333\n",
      "       B-tim       0.95      0.84      0.89       393\n",
      "       I-geo       0.97      0.96      0.97       156\n",
      "       I-gpe       1.00      1.00      1.00         2\n",
      "       I-nat       1.00      1.00      1.00         4\n",
      "       I-org       0.95      0.93      0.94       321\n",
      "       I-per       0.95      0.98      0.96       319\n",
      "       I-tim       1.00      0.86      0.93       108\n",
      "           O       0.99      0.99      0.99     16918\n",
      "\n",
      "    accuracy                           0.97     20000\n",
      "   macro avg       0.80      0.76      0.77     20000\n",
      "weighted avg       0.97      0.97      0.97     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(train_array, train_labels)\n",
    "test_predictions = lin_clf.predict(test_array)\n",
    "report = classification_report(test_labels, test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Performance\n",
    "\n",
    "    Geographical Entities (B-geo and I-geo): The model shows excellent precision and recall for both beginning and inside markers of geographical entities, with F1-scores of 0.85 and 0.97, respectively. This indicates the model's strong capability in recognizing and classifying geographical names accurately.\n",
    "\n",
    "    Temporal Entities (B-tim and I-tim): Temporal entities are also well-handled, with high F1-scores of 0.88 for beginning markers and 0.92 for inside markers. The model effectively identifies and classifies dates and times.\n",
    "\n",
    "    Persons (B-per and I-per): The precision and recall for person names are both high, with F1-scores of 0.81 and 0.96 for beginning and inside markers, respectively. This shows the model's proficiency in recognizing names of people.\n",
    "\n",
    "    General Entities (O): The model achieves near-perfect performance in identifying tokens outside of named entities, with an F1-score of 0.99. This is crucial for NER tasks, as the majority of text often falls under this category.\n",
    "\n",
    "Moderate Performance\n",
    "\n",
    "    Organizations (B-org and I-org): While still good, the performance on organization entities shows room for improvement, with F1-scores of 0.70 for beginning markers and 0.94 for inside markers. The lower score for beginning markers suggests challenges in initially identifying organizations.\n",
    "\n",
    "    Political Entities (B-gpe): The model performs well on geopolitical entities with an F1-score of 0.90, indicating robust identification and classification of countries, cities, and states.\n",
    "\n",
    "Low or No Performance\n",
    "\n",
    "    Art, Events, and Nature (B-art, B-eve, B-nat, I-nat): The model struggles with these categories, showing very low or no performance on art and events, and moderate performance on natural phenomena. The zero scores for B-art and B-eve indicate a failure to correctly identify any instances of these types, likely due to their sparse representation in the dataset.\n",
    "\n",
    "    Miscellaneous Inside Markers (I-gpe): While I-gpe shows a high F1-score, it's based on a very small sample size (only 2 instances), suggesting that the high performance may not be generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
