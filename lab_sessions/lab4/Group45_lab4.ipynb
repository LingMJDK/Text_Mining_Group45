{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('/mnt/sda1/Text_Mining_Group45/lab_sessions/lab4/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "      'bias': 1.0,\n",
    "      'words': token,  # original token\n",
    "      'pos': pos,  # Part Of Speech tag of the token\n",
    "      'word.lower()': token.lower(),  # lower case variant of the token\n",
    "      'word[-3:]': token[-3:],  # suffix of 3 characters\n",
    "      'word[-2:]': token[-2:],  # suffix of 2 characters\n",
    "      'word.isupper()': token.isupper(),  # is the token in uppercase\n",
    "      'word.istitle()': token.istitle(),  # does the token start with a capital letter\n",
    "      'word.isdigit()': token.isdigit(),  # is the token a digit\n",
    "      'postag': pos,  # Part Of Speech tag\n",
    "      'postag[:2]': pos[:2],  # first two characters of the PoS tag\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader('/mnt/sda1/Text_Mining_Group45/lab_sessions/lab4/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "      'bias': 1.0,\n",
    "      'words': token,  # original token\n",
    "      'pos': pos,  # Part Of Speech tag of the token\n",
    "      'word.lower()': token.lower(),  # lower case variant of the token\n",
    "      'word[-3:]': token[-3:],  # suffix of 3 characters\n",
    "      'word[-2:]': token[-2:],  # suffix of 2 characters\n",
    "      'word.isupper()': token.isupper(),  # is the token in uppercase\n",
    "      'word.istitle()': token.istitle(),  # does the token start with a capital letter\n",
    "      'word.isdigit()': token.isdigit(),  # is the token a digit\n",
    "      'postag': pos,  # Part Of Speech tag\n",
    "      'postag[:2]': pos[:2],  # first two characters of the PoS tag\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances TRAINING SET: 203621, Total instances TEST SET: 46435.\n",
      "\n",
      "Label: O, Absolute frequency TRAINING SET: 169578, Relative frequency TRAINING SET: 83.28%.\n",
      "Label: O, Absolute frequency TEST SET: 38323, Relative frequency TEST SET: 82.53%.\n",
      "\n",
      "Label: B-LOC, Absolute frequency TRAINING SET: 7140, Relative frequency TRAINING SET: 3.51%.\n",
      "Label: B-LOC, Absolute frequency TEST SET: 1668, Relative frequency TEST SET: 3.59%.\n",
      "\n",
      "Label: B-PER, Absolute frequency TRAINING SET: 6600, Relative frequency TRAINING SET: 3.24%.\n",
      "Label: B-ORG, Absolute frequency TEST SET: 1661, Relative frequency TEST SET: 3.58%.\n",
      "\n",
      "Label: B-ORG, Absolute frequency TRAINING SET: 6321, Relative frequency TRAINING SET: 3.10%.\n",
      "Label: B-PER, Absolute frequency TEST SET: 1617, Relative frequency TEST SET: 3.48%.\n",
      "\n",
      "Label: I-PER, Absolute frequency TRAINING SET: 4528, Relative frequency TRAINING SET: 2.22%.\n",
      "Label: I-PER, Absolute frequency TEST SET: 1156, Relative frequency TEST SET: 2.49%.\n",
      "\n",
      "Label: I-ORG, Absolute frequency TRAINING SET: 3704, Relative frequency TRAINING SET: 1.82%.\n",
      "Label: I-ORG, Absolute frequency TEST SET: 835, Relative frequency TEST SET: 1.80%.\n",
      "\n",
      "Label: B-MISC, Absolute frequency TRAINING SET: 3438, Relative frequency TRAINING SET: 1.69%.\n",
      "Label: B-MISC, Absolute frequency TEST SET: 702, Relative frequency TEST SET: 1.51%.\n",
      "\n",
      "Label: I-LOC, Absolute frequency TRAINING SET: 1157, Relative frequency TRAINING SET: 0.57%.\n",
      "Label: I-LOC, Absolute frequency TEST SET: 257, Relative frequency TEST SET: 0.55%.\n",
      "\n",
      "Label: I-MISC, Absolute frequency TRAINING SET: 1155, Relative frequency TRAINING SET: 0.57%.\n",
      "Label: I-MISC, Absolute frequency TEST SET: 216, Relative frequency TEST SET: 0.47%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter \n",
    "# count_train=Counter(training_gold_labels)\n",
    "# count_test=Counter(test_gold_labels)\n",
    "\n",
    "# #no of instances\n",
    "# print(\"\\nNumber of instances in training data:\", len(training_gold_labels))\n",
    "# print(\"Number of instances in test data:\", len(test_gold_labels))\n",
    "# print(\"Proportion test data:\", len(test_gold_labels)/(len(test_gold_labels)+len(training_gold_labels)))\n",
    "# print('\\n')\n",
    "# # new_dict={}\n",
    "# # #frequency distribution of NERC labels\n",
    "# print('Train NERC labels')\n",
    "# for label, freq in count_train.items():\n",
    "#     print(f\"{label}: {freq} ({round((freq/len(training_gold_labels))*100,2)}%)\")\n",
    "\n",
    "# print('\\nTest NERC labels')\n",
    "# for label, freq in count_test.items():\n",
    "#     print(f\"{label}: {freq} ({round((freq/len(test_gold_labels))*100, 2)}%)\")\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "num_train:int = len(training_features) # number of instances in the training set\n",
    "num_test:int = len(test_features) # number of instances in the test set\n",
    "\n",
    "freq_dist_train = Counter(training_gold_labels)\n",
    "freq_dist_test = Counter(test_gold_labels)\n",
    "\n",
    "freq_dist_train: Dict[str, int] = dict(sorted(freq_dist_train.items(), key=lambda item: item[1], reverse=True)) # The sorted frequency distribution dictionary of the training set (high to low)\n",
    "freq_dist_test: Dict[str, int] = dict(sorted(freq_dist_test.items(), key=lambda item: item[1], reverse=True)) # The sorted frequency distribution dictionary of the test set (high to low)\n",
    "\n",
    "print(f'Total instances TRAINING SET: {num_train}, Total instances TEST SET: {num_test}.')\n",
    "print()\n",
    "\n",
    "for i,j in zip(freq_dist_train, freq_dist_test):\n",
    "    print(f'Label: {i}, Absolute frequency TRAINING SET: {freq_dist_train[i]}, Relative frequency TRAINING SET: {(freq_dist_train[i]/num_train)*100:.2f}%.')\n",
    "    print(f'Label: {j}, Absolute frequency TEST SET: {freq_dist_test[j]}, Relative frequency TEST SET: {(freq_dist_test[j]/num_test)*100:.2f}%.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer (b)\n",
    "\n",
    "The training and test data show a distribution of labels similar between the two sets. For example, the most common label, 'O', which represents words not identified as named entities, makes up 83.28% of the training set and 82.53% of the test set, showing a slight variation but maintaining a consistent majority. Other labels such as 'B-LOC' (beginning of location), 'B-PER' (beginning of person), and 'B-ORG' (beginning of organization) also show similar proportions between the training and test sets, with 'B-LOC' appearing in 3.51% of the training data and 3.59% in the test data, 'B-PER' making up 3.24% of the training and slightly more, 3.48%, in the test set, and 'B-ORG' comprising 3.10% of the training set compared to 3.58% of the test set. This alignment indicates a deliberate attempt to mirror the label distribution across both datasets to ensure the model's performance can be accurately assessed.\n",
    "\n",
    "However, when examining the balance of instances across the various NERC labels, it's clear there's a significant imbalance, particularly with the overwhelming presence of the 'O' label compared to others. Such dominance suggests that instances of named entities like 'I-PER' (inside of person), 'I-ORG' (inside of organization), 'I-LOC' (inside of location), and 'I-MISC' (inside of miscellaneous) are far less frequent. For instance, 'I-PER' constitutes only 2.22% of the training set and sees a slight increase to 2.49% in the test set, while 'I-MISC' and 'I-LOC' are even less common, each making up less than 1% of the instances in both datasets. This imbalance reflects the distribution of entities in natural language; however, it is also important to ensure that the model is adequately exposed to less frequent entitity types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "features=training_features + test_features\n",
    "the_array = vec.fit_transform(features)\n",
    "# print(the_array)\n",
    "# print(len(the_array))\n",
    "\n",
    "n_train = len(training_features)\n",
    "\n",
    "train_array = the_array[:n_train]\n",
    "test_array = the_array[n_train:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.73      0.81      0.77      1668\n",
      "      B-MISC       0.71      0.70      0.71       702\n",
      "       B-ORG       0.68      0.58      0.63      1661\n",
      "       B-PER       0.68      0.59      0.63      1617\n",
      "       I-LOC       0.59      0.54      0.56       257\n",
      "      I-MISC       0.56      0.59      0.58       216\n",
      "       I-ORG       0.54      0.49      0.51       835\n",
      "       I-PER       0.48      0.54      0.51      1156\n",
      "           O       0.98      0.99      0.99     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.66      0.65      0.65     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "lin_clf.fit(train_array, training_gold_labels)\n",
    "test_predictions = lin_clf.predict(test_array)\n",
    "report = classification_report(test_gold_labels, test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer (d)\n",
    "## Micheal TO DO\n",
    "The classifier demonstrates strong performance particularly on the 'O' (Outside of named entities) label, achieving high precision, recall, and F1-score of 0.98, 0.99, and 0.99 respectively. This label, which represents tokens not classified as named entities, benefits from its overwhelming presence in the dataset, as evidenced by the provided data distribution. Such a dominant representation facilitates the classifier's learning, making it adept at identifying non-entity components of the text with high accuracy. The high performance on 'O' significantly contributes to the overall accuracy metric of 0.93, indicating that the classifier is particularly efficient at discerning non-entity text portions, likely due to the abundance of examples during training that bolster its predictive confidence and reduce the likelihood of false positives or negatives within this category.\n",
    "\n",
    "Conversely, the classifier shows comparatively weaker performance on several entity categories, notably 'I-PER' (Inside a person's name), 'I-ORG' (Inside an organization's name), and 'I-LOC' (Inside a location), with F1-scores of 0.51, 0.51, and 0.56 respectively. These categories are characterized by lower precision and recall, suggesting difficulties in accurately identifying and classifying tokens that are part of named entities extending beyond a single token. The relatively poor performance on these labels can be attributed to several factors, including the inherent complexity of recognizing entities that span multiple tokens, potential inconsistencies in labeling, and the relatively smaller number of examples for these categories compared to the 'O' label. This results in less training data for these specific entity types, complicating the model's task of learning their distinctive features amidst the linguistic variability of natural language. Additionally, the precision-recall trade-off observed, especially in 'I-PER' and 'I-ORG', points to challenges in balancing the detection of true positives against the avoidance of false positives, further complicating the accurate classification of these entity types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.80      0.78      1668\n",
      "      B-MISC       0.72      0.70      0.71       702\n",
      "       B-ORG       0.69      0.64      0.66      1661\n",
      "       B-PER       0.75      0.67      0.71      1617\n",
      "       I-LOC       0.51      0.42      0.46       257\n",
      "      I-MISC       0.60      0.54      0.57       216\n",
      "       I-ORG       0.48      0.33      0.39       835\n",
      "       I-PER       0.59      0.50      0.54      1156\n",
      "           O       0.97      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.62      0.64     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "##### Adapt the path to point to your local copy of the Google embeddings model\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/mnt/sda1/Text_Mining_Group45/lab_sessions/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "training_vectors=[]\n",
    "train_labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        training_vectors.append(vector)\n",
    "        train_labels.append(ne_label)\n",
    "\n",
    "test_vectors=[]\n",
    "test_labels=[]\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        test_vectors.append(vector)\n",
    "        test_labels.append(ne_label)\n",
    "\n",
    "lin_clf2 = svm.LinearSVC()\n",
    "lin_clf2.fit(training_vectors, train_labels)\n",
    "pred=lin_clf2.predict(test_vectors)\n",
    "\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remi TO DO\n",
    "\n",
    "The comparison between the classification results using manually crafted feature vectors and those obtained from embedding-based features shows a notable improvement in the model's performance when leveraging word embeddings. Specifically, for entity types such as B-LOC, B-MISC, B-ORG, and B-PER, there is a marked increase in precision and recall, leading to higher F1-scores. This improvement underscores the power of embeddings to capture semantic and contextual nuances of words, enhancing the model's ability to distinguish between different entity types more effectively. While the accuracy for non-entity type 'O' remains high in both cases, the use of embeddings has contributed to a more balanced performance across all entity types, as evidenced by the increase in macro-average F1-score from 0.65 to 0.74. This highlights the embeddings' role in providing a richer representation of tokens, facilitating better generalization and more nuanced entity recognition compared to traditional feature engineering approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = '/mnt/sda1/Text_Mining_Group45/lab_sessions/lab4/kaggle/ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050795"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16896/2272784935.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['sentence_idx']=new\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>...</th>\n",
       "      <th>prev-prev-lemma</th>\n",
       "      <th>prev-prev-pos</th>\n",
       "      <th>prev-prev-shape</th>\n",
       "      <th>prev-prev-word</th>\n",
       "      <th>prev-shape</th>\n",
       "      <th>prev-word</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>shape</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thousand</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>...</td>\n",
       "      <td>__start2__</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>1.0</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>...</td>\n",
       "      <td>__start1__</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__START1__</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>...</td>\n",
       "      <td>thousand</td>\n",
       "      <td>NNS</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>have</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>VBN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>of</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>march</td>\n",
       "      <td>through</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>capitalized</td>\n",
       "      <td>London</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>through</td>\n",
       "      <td>...</td>\n",
       "      <td>demonstr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>have</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     lemma next-lemma next-next-lemma next-next-pos next-next-shape  \\\n",
       "0   0  thousand         of        demonstr           NNS       lowercase   \n",
       "1   1        of   demonstr            have           VBP       lowercase   \n",
       "2   2  demonstr       have           march           VBN       lowercase   \n",
       "3   3      have      march         through            IN       lowercase   \n",
       "4   4     march    through          london           NNP     capitalized   \n",
       "\n",
       "  next-next-word next-pos next-shape      next-word  ... prev-prev-lemma  \\\n",
       "0  demonstrators       IN  lowercase             of  ...      __start2__   \n",
       "1           have      NNS  lowercase  demonstrators  ...      __start1__   \n",
       "2        marched      VBP  lowercase           have  ...        thousand   \n",
       "3        through      VBN  lowercase        marched  ...              of   \n",
       "4         London       IN  lowercase        through  ...        demonstr   \n",
       "\n",
       "  prev-prev-pos prev-prev-shape prev-prev-word   prev-shape      prev-word  \\\n",
       "0    __START2__        wildcard     __START2__     wildcard     __START1__   \n",
       "1    __START1__        wildcard     __START1__  capitalized      Thousands   \n",
       "2           NNS     capitalized      Thousands    lowercase             of   \n",
       "3            IN       lowercase             of    lowercase  demonstrators   \n",
       "4           NNS       lowercase  demonstrators    lowercase           have   \n",
       "\n",
       "  sentence_idx        shape           word tag  \n",
       "0          1.0  capitalized      Thousands   O  \n",
       "1          1.0    lowercase             of   O  \n",
       "2          1.0    lowercase  demonstrators   O  \n",
       "3          1.0    lowercase           have   O  \n",
       "4          1.0    lowercase        marched   O  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "\n",
    "sentence_column=df_train['sentence_idx']\n",
    "\n",
    "def string(x):\n",
    "    return (str(x))\n",
    "\n",
    "new=sentence_column.apply(string)\n",
    "df_train['sentence_idx']=new\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split df into labels and features arrays \n",
    "\n",
    "train_labels = df_train['tag'].values\n",
    "features_train = df_train.drop('tag', axis=1)\n",
    "features_train = features_train.drop('id', axis=1)\n",
    "# features_train = features_train.drop('sentence_idx', axis=1)\n",
    "features_dict_train = features_train.to_dict(orient='records')\n",
    "\n",
    "test_labels = df_test['tag'].values\n",
    "features_test = df_test.drop('tag', axis=1)\n",
    "features_test = features_test.drop('id', axis=1)\n",
    "# features_test = features_test.drop('sentence_idx', axis=1)\n",
    "features_dict_test = features_test.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn import svm\n",
    "# from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemma': 'of', 'next-lemma': 'demonstr', 'next-next-lemma': 'have', 'next-next-pos': 'VBP', 'next-next-shape': 'lowercase', 'next-next-word': 'have', 'next-pos': 'NNS', 'next-shape': 'lowercase', 'next-word': 'demonstrators', 'pos': 'IN', 'prev-iob': 'O', 'prev-lemma': 'thousand', 'prev-pos': 'NNS', 'prev-prev-iob': '__START1__', 'prev-prev-lemma': '__start1__', 'prev-prev-pos': '__START1__', 'prev-prev-shape': 'wildcard', 'prev-prev-word': '__START1__', 'prev-shape': 'capitalized', 'prev-word': 'Thousands', 'sentence_idx': '1.0', 'shape': 'lowercase', 'word': 'of'}\n",
      "  (0, 7329)\t1.0\n",
      "  (0, 13352)\t1.0\n",
      "  (0, 18437)\t1.0\n",
      "  (0, 24093)\t1.0\n",
      "  (0, 24122)\t1.0\n",
      "  (0, 29674)\t1.0\n",
      "  (0, 35348)\t1.0\n",
      "  (0, 35386)\t1.0\n",
      "  (0, 43763)\t1.0\n",
      "  (0, 46843)\t1.0\n",
      "  (0, 46883)\t1.0\n",
      "  (0, 47411)\t1.0\n",
      "  (0, 55054)\t1.0\n",
      "  (0, 55074)\t1.0\n",
      "  (0, 55577)\t1.0\n",
      "  (0, 62960)\t1.0\n",
      "  (0, 62973)\t1.0\n",
      "  (0, 67110)\t1.0\n",
      "  (0, 74411)\t1.0\n",
      "  (0, 78696)\t1.0\n",
      "  (0, 86270)\t1.0\n",
      "  (0, 90816)\t1.0\n",
      "  (0, 94729)\t1.0\n",
      "  (1, 5280)\t1.0\n",
      "  (1, 10446)\t1.0\n",
      "  :\t:\n",
      "  (119998, 90819)\t1.0\n",
      "  (119998, 99619)\t1.0\n",
      "  (119999, 4721)\t1.0\n",
      "  (119999, 11904)\t1.0\n",
      "  (119999, 21330)\t1.0\n",
      "  (119999, 24081)\t1.0\n",
      "  (119999, 24122)\t1.0\n",
      "  (119999, 32344)\t1.0\n",
      "  (119999, 35371)\t1.0\n",
      "  (119999, 35386)\t1.0\n",
      "  (119999, 42679)\t1.0\n",
      "  (119999, 46843)\t1.0\n",
      "  (119999, 46882)\t1.0\n",
      "  (119999, 52165)\t1.0\n",
      "  (119999, 55023)\t1.0\n",
      "  (119999, 55072)\t1.0\n",
      "  (119999, 60311)\t1.0\n",
      "  (119999, 62934)\t1.0\n",
      "  (119999, 62967)\t1.0\n",
      "  (119999, 71620)\t1.0\n",
      "  (119999, 74405)\t1.0\n",
      "  (119999, 83207)\t1.0\n",
      "  (119999, 86269)\t5469.0\n",
      "  (119999, 90819)\t1.0\n",
      "  (119999, 99202)\t1.0\n",
      "(100000, 102681)\n",
      "(20000, 102681)\n"
     ]
    }
   ],
   "source": [
    "vec2 = DictVectorizer()\n",
    "features2=features_dict_train + features_dict_test\n",
    "print(features2[1])\n",
    "the_array = vec2.fit_transform(features2)\n",
    "print(the_array)\n",
    "\n",
    "n_train = len(features_dict_train)\n",
    "\n",
    "train_array = the_array[:n_train]\n",
    "test_array = the_array[n_train:]\n",
    "\n",
    "print(train_array.shape)\n",
    "print(test_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         4\n",
      "       B-eve       0.00      0.00      0.00         0\n",
      "       B-geo       0.87      0.88      0.87       741\n",
      "       B-gpe       0.90      0.93      0.92       296\n",
      "       B-nat       0.80      0.50      0.62         8\n",
      "       B-org       0.77      0.67      0.72       397\n",
      "       B-per       0.81      0.83      0.82       333\n",
      "       B-tim       0.95      0.84      0.89       393\n",
      "       I-geo       0.97      0.96      0.97       156\n",
      "       I-gpe       1.00      1.00      1.00         2\n",
      "       I-nat       1.00      1.00      1.00         4\n",
      "       I-org       0.95      0.93      0.94       321\n",
      "       I-per       0.95      0.98      0.96       319\n",
      "       I-tim       1.00      0.86      0.93       108\n",
      "           O       0.99      0.99      0.99     16918\n",
      "\n",
      "    accuracy                           0.97     20000\n",
      "   macro avg       0.80      0.76      0.77     20000\n",
      "weighted avg       0.97      0.97      0.97     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(train_array, train_labels)\n",
    "test_predictions = lin_clf.predict(test_array)\n",
    "report = classification_report(test_labels, test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B-art (Beginning of Art): Shows very low performance with all metrics at 0.00, indicating that the model failed to correctly identify any 'Art' entities. This could be due to very low sample size (support = 4) or lack of relevant features for these entities in the training data.\n",
    "\n",
    "B-eve (Beginning of Event): No instances in the sample, which means the model did not have a chance to predict this category in the dataset provided.\n",
    "\n",
    "B-geo (Beginning of Geographical Entity): High precision (0.87) and recall (0.88) leading to a high f1-score (0.87), indicating strong performance in identifying geographical entities, likely due to a larger support (741).\n",
    "\n",
    "B-gpe (Beginning of Geopolitical Entity): Very good performance with precision at 0.90, recall at 0.93, and f1-score at 0.92, supported by a significant number of instances (296). This suggests the model is well-tuned for recognizing geopolitical entities.\n",
    "\n",
    "B-nat (Beginning of Natural Phenomenon): Moderate performance with precision at 0.80, recall at 0.50, and f1-score at 0.62. This suggests the model can recognize natural phenomena to some extent but struggles with consistency, likely due to very low support (8).\n",
    "\n",
    "B-org (Beginning of Organization): Good performance with precision at 0.77, recall at 0.67, and f1-score at 0.72, indicating the model is fairly reliable at identifying organizations from the text.\n",
    "\n",
    "B-per (Beginning of Person): Strong performance with precision at 0.81, recall at 0.83, and f1-score at 0.82. This shows the model is effective at identifying individuals' names.\n",
    "\n",
    "B-tim (Beginning of Time Expression): Excellent precision (0.95) and good recall (0.84) leading to a high f1-score (0.89), indicating strong performance in recognizing time-related expressions.\n",
    "\n",
    "I-geo (Inside Geographical Entity), I-gpe (Inside Geopolitical Entity), I-nat (Inside Natural Phenomenon), I-org (Inside Organization), I-per (Inside Person), and I-tim (Inside Time Expression): These 'Inside' labels generally show high precision, recall, and f1-scores, indicating that the model is very effective at continuing to recognize entities once they have been identified as beginning. Notably, I-gpe and I-nat have perfect scores, but their support is very low, suggesting limited testing instances.\n",
    "\n",
    "O (Outside of Named Entity): Almost perfect scores in precision, recall, and f1-score (all approximately 0.99), supported by a very large number of instances (16918). This suggests the model is extremely effective at identifying tokens that are not named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
